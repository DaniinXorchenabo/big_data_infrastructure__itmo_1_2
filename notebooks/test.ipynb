{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:54:29.103471Z",
     "start_time": "2025-02-26T12:54:26.937315Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install kagglehub[pandas-datasets]",
   "id": "705e6d5acc612d0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.11/site-packages (0.3.10)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (24.2)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (4.67.1)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas->kagglehub[pandas-datasets]) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->kagglehub[pandas-datasets]) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->kagglehub[pandas-datasets]) (2025.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->kagglehub[pandas-datasets]) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->kagglehub[pandas-datasets]) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->kagglehub[pandas-datasets]) (1.26.20)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->kagglehub[pandas-datasets]) (2025.1.31)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-01T09:26:20.934614Z",
     "start_time": "2025-03-01T09:26:19.918792Z"
    }
   },
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %reload_ext autoreload\n",
    "\n",
    "import os\n",
    "ROOT_DIR = '/workspace/NN'\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "import shutil\n",
    "import kagglehub\n",
    "import torch\n",
    "\n",
    "dataset_path = os.path.join(ROOT_DIR, 'neural', 'datasets', 'fashionmnist')\n",
    "logs_path = os.path.join(ROOT_DIR, 'neural', 'logs')\n",
    "csv_file = os.path.join(dataset_path, 'fashion-mnist_train.csv')\n",
    "prod_weights_path = os.path.join(ROOT_DIR, 'neural', 'weights', 'prod')\n",
    "test_weights_path = os.path.join(ROOT_DIR, 'neural', 'weights', 'lab_1')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T09:26:30.724591Z",
     "start_time": "2025-03-01T09:26:28.688416Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# Определяем лёгкую сверточную сеть для FashionMNIST\n",
    "class LightweightFashionMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LightweightFashionMNIST, self).__init__()\n",
    "        # Входное изображение: 1 x 28 x 28\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)   # -> 16 x 28 x 28\n",
    "        self.pool  = nn.MaxPool2d(2, 2)                             # -> 16 x 14 x 14\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)    # -> 32 x 14 x 14\n",
    "        # После второго pooling: 32 x 7 x 7\n",
    "        self.fc1   = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.fc2   = nn.Linear(64, 10)  # 10 классов\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Определяем LightningModule\n",
    "class FashionMNISTLitModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super(FashionMNISTLitModel, self).__init__()\n",
    "        self.model = LightweightFashionMNIST()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Метрики для обучения и валидации\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='multiclass',num_classes=10)\n",
    "        self.val_precision = torchmetrics.Precision(task='multiclass',num_classes=10, average='macro')\n",
    "        self.val_recall = torchmetrics.Recall(task='multiclass', num_classes=10, average='macro')\n",
    "        self.val_auroc = torchmetrics.AUROC(task='multiclass',num_classes=10)\n",
    "        self.confmat = torchmetrics.ConfusionMatrix(task='multiclass', num_classes=10)\n",
    "        self.val_f1 = MulticlassF1Score( num_classes=10, average='macro')  # Добавлена метрика F1\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_accuracy.update(preds, y)\n",
    "        self.log('train/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # Вместо training_epoch_end, используем on_train_epoch_end (без аргументов)\n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.log('train/epoch_time', epoch_time)\n",
    "        acc = self.train_accuracy.compute()\n",
    "        self.log('train/accuracy', acc, prog_bar=True)\n",
    "        self.train_accuracy.reset()\n",
    "        self.log('train/lr', self.learning_rate)\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_accuracy.update(preds, y)\n",
    "        self.val_precision.update(preds, y)\n",
    "        self.val_recall.update(preds, y)\n",
    "        self.val_auroc.update(F.softmax(logits, dim=1), y)\n",
    "        self.val_f1.update(preds, y)  # Обновление F1 метрики\n",
    "\n",
    "        self.confmat.update(preds, y)\n",
    "        self.log('val/loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        acc = self.val_accuracy.compute()\n",
    "        prec = self.val_precision.compute()\n",
    "        rec = self.val_recall.compute()\n",
    "        f1 = self.val_f1.compute()  # Вычисление F1 метрики\n",
    "        auroc = self.val_auroc.compute()\n",
    "        self.log('val/accuracy', acc, prog_bar=True)\n",
    "        self.log('val/precision', prec)\n",
    "        self.log('val/recall', rec)\n",
    "        self.log('val/auroc', auroc)\n",
    "        self.log('val/f1', f1)  # Логирование F1 метрики\n",
    "\n",
    "        # Логируем матрицу ошибок как изображение в TensorBoard\n",
    "        confmat = self.confmat.compute().cpu().numpy()\n",
    "        fig = self.plot_confusion_matrix(confmat)\n",
    "        self.logger.experiment.add_figure(\"Confusion Matrix\", fig, self.current_epoch)\n",
    "\n",
    "        self.val_accuracy.reset()\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.val_auroc.reset()\n",
    "        self.val_f1.reset()  # Сброс F1 метрики\n",
    "        self.confmat.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def plot_confusion_matrix(self, confmat):\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        im = ax.imshow(confmat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        ax.set(xticks=np.arange(confmat.shape[1]),\n",
    "               yticks=np.arange(confmat.shape[0]),\n",
    "               xticklabels=np.arange(10),\n",
    "               yticklabels=np.arange(10),\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label',\n",
    "               title='Confusion Matrix')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        thresh = confmat.max() / 2.\n",
    "        for i in range(confmat.shape[0]):\n",
    "            for j in range(confmat.shape[1]):\n",
    "                ax.text(j, i, format(confmat[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if confmat[i, j] > thresh else \"black\")\n",
    "        fig.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Подготовка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = FashionMNIST(root=dataset_path, train=True, download=True, transform=transform)\n",
    "val_dataset   = FashionMNIST(root=dataset_path, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d6dc0e65c5043d0c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Инициализируем модель\n",
    "learn_id = str(uuid.uuid4())\n",
    "model_name = 'fashion_MNIST_lite'\n",
    "model = FashionMNISTLitModel(learning_rate=1e-3)\n",
    "# Настраиваем TensorBoard логгер\n",
    "logger = TensorBoardLogger(\n",
    "        logs_path,\n",
    "        name=model_name,\n",
    "        version=learn_id,\n",
    "        sub_dir=(_sd:=f\"{(u_:=datetime.utcnow().strftime('%Y_%m_%d__%H_%M_%S'))}\"),\n",
    ")\n",
    "# Настраиваем колбэк для сохранения модели\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=(_tg_m:='val/auroc'),       # Метрика для мониторинга\n",
    "    dirpath=test_weights_path, # Каталог для сохранения моделей\n",
    "    filename=f'{model_name}_{_sd}_{learn_id}' + '-{epoch:02d}-{' + _tg_m + ':.2f}', # Имя файла\n",
    "    save_top_k=1,             # Сохранять только лучшую модель\n",
    "    mode='max'                # Минимизировать monitored метрику\n",
    ")\n",
    "\n",
    "# Создаем тренер PyTorch Lightning\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "# Запускаем обучение\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "id": "4505642880511b87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T11:42:01.458545Z",
     "start_time": "2025-02-27T11:42:01.335382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "\n",
    "target_path_to_model = os.path.join(prod_weights_path, os.path.split(os.path.split(best_model_path)[0])[1] + '_' + os.path.split(best_model_path)[1])\n",
    "\n",
    "shutil.move(best_model_path, target_path_to_model)\n"
   ],
   "id": "ad0d59906d598353",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/NN/my_checkpoints/fashion_MNIST_lite_2025_02_27__11_35_34_af5d8943-4cf8-4204-b7a3-f39e5a49e673-epoch=10-val/auroc=0.99.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.11/shutil.py:853\u001B[0m, in \u001B[0;36mmove\u001B[0;34m(src, dst, copy_function)\u001B[0m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 853\u001B[0m     os\u001B[38;5;241m.\u001B[39mrename(src, real_dst)\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/workspace/NN/my_checkpoints/fashion_MNIST_lite_2025_02_27__11_35_34_af5d8943-4cf8-4204-b7a3-f39e5a49e673-epoch=10-val/auroc=0.99.ckpt' -> '/workspace/NN/neural/prod'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m best_model_path \u001B[38;5;241m=\u001B[39m checkpoint_callback\u001B[38;5;241m.\u001B[39mbest_model_path\n\u001B[0;32m----> 2\u001B[0m \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmove\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbest_model_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprod_weights_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m path_to_model \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(prod_weights_path, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(best_model_path)[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m      4\u001B[0m trained_model \u001B[38;5;241m=\u001B[39m FashionMNISTLitModel\u001B[38;5;241m.\u001B[39mload_from_checkpoint(\n\u001B[1;32m      5\u001B[0m     checkpoint_path\u001B[38;5;241m=\u001B[39mpath_to_model,\n\u001B[1;32m      6\u001B[0m     map_location\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(device\u001B[38;5;241m=\u001B[39mdevice)  \u001B[38;5;66;03m# или 'cuda' для GPU\u001B[39;00m\n\u001B[1;32m      7\u001B[0m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/shutil.py:873\u001B[0m, in \u001B[0;36mmove\u001B[0;34m(src, dst, copy_function)\u001B[0m\n\u001B[1;32m    871\u001B[0m         rmtree(src)\n\u001B[1;32m    872\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 873\u001B[0m         \u001B[43mcopy_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_dst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    874\u001B[0m         os\u001B[38;5;241m.\u001B[39munlink(src)\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m real_dst\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/shutil.py:448\u001B[0m, in \u001B[0;36mcopy2\u001B[0;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(dst):\n\u001B[1;32m    447\u001B[0m     dst \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dst, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(src))\n\u001B[0;32m--> 448\u001B[0m \u001B[43mcopyfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_symlinks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_symlinks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    449\u001B[0m copystat(src, dst, follow_symlinks\u001B[38;5;241m=\u001B[39mfollow_symlinks)\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dst\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/shutil.py:256\u001B[0m, in \u001B[0;36mcopyfile\u001B[0;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[1;32m    254\u001B[0m     os\u001B[38;5;241m.\u001B[39msymlink(os\u001B[38;5;241m.\u001B[39mreadlink(src), dst)\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(src, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fsrc:\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    258\u001B[0m             \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(dst, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fdst:\n\u001B[1;32m    259\u001B[0m                 \u001B[38;5;66;03m# macOS\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/workspace/NN/my_checkpoints/fashion_MNIST_lite_2025_02_27__11_35_34_af5d8943-4cf8-4204-b7a3-f39e5a49e673-epoch=10-val/auroc=0.99.ckpt'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:07:07.939354Z",
     "start_time": "2025-02-27T14:07:07.868894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_path_to_model = '/workspace/NN/neural/weights/prod/fashion_MNIST_lite_2025_02_27__11_35_34_af5d8943-4cf8-4204-b7a3-f39e5a49e673-epoch=10-val_auroc=0.99.ckpt'\n",
    "trained_model = FashionMNISTLitModel.load_from_checkpoint(\n",
    "    checkpoint_path=target_path_to_model,\n",
    "    map_location=torch.device(device=device)  # или 'cuda' для GPU\n",
    ")\n",
    "model_name='fashion_MNIST_lite'\n",
    "learn_id = 'af5d8943-4cf8-4204-b7a3-f39e5a49e673'\n",
    "torch.save(trained_model.model.state_dict(), os.path.join(os.path.split(target_path_to_model)[0], f'{model_name}_{learn_id}.pth'))"
   ],
   "id": "d0032a0d8dd89056",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T09:35:17.048668Z",
     "start_time": "2025-03-01T09:35:16.045427Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 11,
   "source": [
    "from src.configs.config import CONFIG\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if True or os.path.isfile(CONFIG.MODEL_DIR):\n",
    "    model_path = hf_hub_download(repo_id=CONFIG.AI_WEIGHTS_REPO, filename=CONFIG.AI_WEIGHTS_REPO_FILENAME, force_download=True, local_dir= os.path.join(ROOT_DIR, 'neural', 'weights',  'prod'))\n",
    "    source_dir = model_path\n",
    "    target_dir = CONFIG.MODEL_DIR\n",
    "    # shutil.move(source_dir, target_dir)"
   ],
   "id": "a1dbef0b842bdc0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:21:19.007239Z",
     "start_time": "2025-03-09T16:21:13.849031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!apt-get update\n",
    "!apt-get install -y libsasl2-dev python3-dev gcc g++\n",
    "!pip install sasl\n",
    "!pip install happybase thrift_sasl sasl thrift\n"
   ],
   "id": "7f9ede338af46d1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://deb.debian.org/debian bullseye InRelease\r\n",
      "Hit:3 http://deb.debian.org/debian-security bullseye-security InRelease        \r\n",
      "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64  InRelease\r\n",
      "Hit:4 http://deb.debian.org/debian bullseye-updates InRelease\r\n",
      "Reading package lists... Done\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree... Done\r\n",
      "Reading state information... Done\r\n",
      "libsasl2-dev is already the newest version (2.1.27+dfsg-2.1+deb11u1).\r\n",
      "g++ is already the newest version (4:10.2.1-1).\r\n",
      "gcc is already the newest version (4:10.2.1-1).\r\n",
      "python3-dev is already the newest version (3.9.2-3).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 27 not upgraded.\r\n",
      "Collecting sasl\r\n",
      "  Using cached sasl-0.3.1.tar.gz (44 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from sasl) (1.17.0)\r\n",
      "Building wheels for collected packages: sasl\r\n",
      "  Building wheel for sasl (setup.py) ... \u001B[?25lerror\r\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[31m×\u001B[0m \u001B[32mpython setup.py bdist_wheel\u001B[0m did not run successfully.\r\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[26 lines of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m running bdist_wheel\r\n",
      "  \u001B[31m   \u001B[0m running build\r\n",
      "  \u001B[31m   \u001B[0m running build_py\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m copying sasl/__init__.py -> build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m running egg_info\r\n",
      "  \u001B[31m   \u001B[0m writing sasl.egg-info/PKG-INFO\r\n",
      "  \u001B[31m   \u001B[0m writing dependency_links to sasl.egg-info/dependency_links.txt\r\n",
      "  \u001B[31m   \u001B[0m writing requirements to sasl.egg-info/requires.txt\r\n",
      "  \u001B[31m   \u001B[0m writing top-level names to sasl.egg-info/top_level.txt\r\n",
      "  \u001B[31m   \u001B[0m reading manifest file 'sasl.egg-info/SOURCES.txt'\r\n",
      "  \u001B[31m   \u001B[0m reading manifest template 'MANIFEST.in'\r\n",
      "  \u001B[31m   \u001B[0m adding license file 'LICENSE.txt'\r\n",
      "  \u001B[31m   \u001B[0m writing manifest file 'sasl.egg-info/SOURCES.txt'\r\n",
      "  \u001B[31m   \u001B[0m copying sasl/saslwrapper.cpp -> build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m copying sasl/saslwrapper.h -> build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m copying sasl/saslwrapper.pyx -> build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m running build_ext\r\n",
      "  \u001B[31m   \u001B[0m building 'sasl.saslwrapper' extension\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m g++ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -Isasl -I/usr/local/include/python3.11 -c sasl/saslwrapper.cpp -o build/temp.linux-x86_64-cpython-311/sasl/saslwrapper.o\r\n",
      "  \u001B[31m   \u001B[0m sasl/saslwrapper.cpp:196:12: fatal error: longintrepr.h: No such file or directory\r\n",
      "  \u001B[31m   \u001B[0m   196 |   #include \"longintrepr.h\"\r\n",
      "  \u001B[31m   \u001B[0m       |            ^~~~~~~~~~~~~~~\r\n",
      "  \u001B[31m   \u001B[0m compilation terminated.\r\n",
      "  \u001B[31m   \u001B[0m error: command '/usr/bin/g++' failed with exit code 1\r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001B[31m  ERROR: Failed building wheel for sasl\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[?25h  Running setup.py clean for sasl\r\n",
      "Failed to build sasl\r\n",
      "\u001B[31mERROR: Could not build wheels for sasl, which is required to install pyproject.toml-based projects\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Collecting happybase\r\n",
      "  Using cached happybase-1.2.0-py2.py3-none-any.whl\r\n",
      "Collecting thrift_sasl\r\n",
      "  Using cached thrift_sasl-0.4.3-py2.py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Collecting sasl\r\n",
      "  Using cached sasl-0.3.1.tar.gz (44 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting thrift\r\n",
      "  Using cached thrift-0.21.0-cp311-cp311-linux_x86_64.whl\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/site-packages (from happybase) (1.17.0)\r\n",
      "Collecting thriftpy2>=0.4 (from happybase)\r\n",
      "  Using cached thriftpy2-0.5.2-cp311-cp311-linux_x86_64.whl\r\n",
      "Collecting pure-sasl>=0.6.2 (from thrift_sasl)\r\n",
      "  Using cached pure_sasl-0.6.2-py3-none-any.whl\r\n",
      "Collecting Cython>=3.0.10 (from thriftpy2>=0.4->happybase)\r\n",
      "  Using cached Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\r\n",
      "Collecting ply<4.0,>=3.4 (from thriftpy2>=0.4->happybase)\r\n",
      "  Using cached ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\r\n",
      "Using cached thrift_sasl-0.4.3-py2.py3-none-any.whl (8.3 kB)\r\n",
      "Using cached Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\r\n",
      "Building wheels for collected packages: sasl\r\n",
      "  Building wheel for sasl (setup.py) ... \u001B[?25lerror\r\n",
      "  \u001B[1;31merror\u001B[0m: \u001B[1msubprocess-exited-with-error\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[31m×\u001B[0m \u001B[32mpython setup.py bdist_wheel\u001B[0m did not run successfully.\r\n",
      "  \u001B[31m│\u001B[0m exit code: \u001B[1;36m1\u001B[0m\r\n",
      "  \u001B[31m╰─>\u001B[0m \u001B[31m[26 lines of output]\u001B[0m\r\n",
      "  \u001B[31m   \u001B[0m running bdist_wheel\r\n",
      "  \u001B[31m   \u001B[0m running build\r\n",
      "  \u001B[31m   \u001B[0m running build_py\r\n",
      "  \u001B[31m   \u001B[0m creating build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m copying sasl/__init__.py -> build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m running egg_info\r\n",
      "  \u001B[31m   \u001B[0m writing sasl.egg-info/PKG-INFO\r\n",
      "  \u001B[31m   \u001B[0m writing dependency_links to sasl.egg-info/dependency_links.txt\r\n",
      "  \u001B[31m   \u001B[0m writing requirements to sasl.egg-info/requires.txt\r\n",
      "  \u001B[31m   \u001B[0m writing top-level names to sasl.egg-info/top_level.txt\r\n",
      "  \u001B[31m   \u001B[0m reading manifest file 'sasl.egg-info/SOURCES.txt'\r\n",
      "  \u001B[31m   \u001B[0m reading manifest template 'MANIFEST.in'\r\n",
      "  \u001B[31m   \u001B[0m adding license file 'LICENSE.txt'\r\n",
      "  \u001B[31m   \u001B[0m writing manifest file 'sasl.egg-info/SOURCES.txt'\r\n",
      "  \u001B[31m   \u001B[0m copying sasl/saslwrapper.cpp -> build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m copying sasl/saslwrapper.h -> build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m copying sasl/saslwrapper.pyx -> build/lib.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m running build_ext\r\n",
      "  \u001B[31m   \u001B[0m building 'sasl.saslwrapper' extension\r\n",
      "  \u001B[31m   \u001B[0m creating build/temp.linux-x86_64-cpython-311/sasl\r\n",
      "  \u001B[31m   \u001B[0m g++ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -Isasl -I/usr/local/include/python3.11 -c sasl/saslwrapper.cpp -o build/temp.linux-x86_64-cpython-311/sasl/saslwrapper.o\r\n",
      "  \u001B[31m   \u001B[0m sasl/saslwrapper.cpp:196:12: fatal error: longintrepr.h: No such file or directory\r\n",
      "  \u001B[31m   \u001B[0m   196 |   #include \"longintrepr.h\"\r\n",
      "  \u001B[31m   \u001B[0m       |            ^~~~~~~~~~~~~~~\r\n",
      "  \u001B[31m   \u001B[0m compilation terminated.\r\n",
      "  \u001B[31m   \u001B[0m error: command '/usr/bin/g++' failed with exit code 1\r\n",
      "  \u001B[31m   \u001B[0m \u001B[31m[end of output]\u001B[0m\r\n",
      "  \r\n",
      "  \u001B[1;35mnote\u001B[0m: This error originates from a subprocess, and is likely not a problem with pip.\r\n",
      "\u001B[31m  ERROR: Failed building wheel for sasl\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\u001B[?25h  Running setup.py clean for sasl\r\n",
      "Failed to build sasl\r\n",
      "\u001B[31mERROR: Could not build wheels for sasl, which is required to install pyproject.toml-based projects\u001B[0m\u001B[31m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T16:52:58.949369Z",
     "start_time": "2025-03-09T16:51:16.318021Z"
    }
   },
   "cell_type": "code",
   "source": [
    "! apt-get update\n",
    "! apt-get install libkrb5-dev -y\n",
    "!pip install requests requests-kerberos\n"
   ],
   "id": "e2f78e733e6a85cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:2 http://deb.debian.org/debian bullseye InRelease\r\n",
      "Get:3 http://deb.debian.org/debian-security bullseye-security InRelease [27.2 kB]\r\n",
      "Get:4 http://deb.debian.org/debian bullseye-updates InRelease [44.1 kB]\r\n",
      "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/debian11/x86_64  InRelease\r\n",
      "Get:5 http://deb.debian.org/debian-security bullseye-security/main amd64 Packages [350 kB]\r\n",
      "Fetched 421 kB in 1s (309 kB/s)   \r\n",
      "Reading package lists... Done\r\n",
      "Reading package lists... Done\r\n",
      "Building dependency tree... Done\r\n",
      "Reading state information... Done\r\n",
      "The following additional packages will be installed:\r\n",
      "  comerr-dev krb5-multidev libcom-err2 libgssapi-krb5-2 libgssrpc4\r\n",
      "  libk5crypto3 libkadm5clnt-mit12 libkadm5srv-mit12 libkdb5-10 libkrb5-3\r\n",
      "  libkrb5support0\r\n",
      "Suggested packages:\r\n",
      "  doc-base krb5-doc krb5-user\r\n",
      "Recommended packages:\r\n",
      "  krb5-locales\r\n",
      "The following NEW packages will be installed:\r\n",
      "  comerr-dev krb5-multidev libgssrpc4 libkadm5clnt-mit12 libkadm5srv-mit12\r\n",
      "  libkdb5-10 libkrb5-dev\r\n",
      "The following packages will be upgraded:\r\n",
      "  libcom-err2 libgssapi-krb5-2 libk5crypto3 libkrb5-3 libkrb5support0\r\n",
      "5 upgraded, 7 newly installed, 0 to remove and 23 not upgraded.\r\n",
      "Need to get 1423 kB of archives.\r\n",
      "After this operation, 2135 kB of additional disk space will be used.\r\n",
      "Get:1 http://deb.debian.org/debian-security bullseye-security/main amd64 libcom-err2 amd64 1.46.2-2+deb11u1 [74.3 kB]\r\n",
      "Get:2 http://deb.debian.org/debian-security bullseye-security/main amd64 libk5crypto3 amd64 1.18.3-6+deb11u6 [114 kB]\r\n",
      "Get:3 http://deb.debian.org/debian-security bullseye-security/main amd64 libkrb5support0 amd64 1.18.3-6+deb11u6 [65.8 kB]\r\n",
      "Get:4 http://deb.debian.org/debian-security bullseye-security/main amd64 libkrb5-3 amd64 1.18.3-6+deb11u6 [363 kB]\r\n",
      "Get:5 http://deb.debian.org/debian-security bullseye-security/main amd64 libgssapi-krb5-2 amd64 1.18.3-6+deb11u6 [166 kB]\r\n",
      "Get:6 http://deb.debian.org/debian-security bullseye-security/main amd64 comerr-dev amd64 2.1-1.46.2-2+deb11u1 [107 kB]\r\n",
      "Get:7 http://deb.debian.org/debian-security bullseye-security/main amd64 libgssrpc4 amd64 1.18.3-6+deb11u6 [92.0 kB]\r\n",
      "Get:8 http://deb.debian.org/debian-security bullseye-security/main amd64 libkdb5-10 amd64 1.18.3-6+deb11u6 [73.8 kB]\r\n",
      "Get:9 http://deb.debian.org/debian-security bullseye-security/main amd64 libkadm5srv-mit12 amd64 1.18.3-6+deb11u6 [86.3 kB]\r\n",
      "Get:10 http://deb.debian.org/debian-security bullseye-security/main amd64 libkadm5clnt-mit12 amd64 1.18.3-6+deb11u6 [74.4 kB]\r\n",
      "Get:11 http://deb.debian.org/debian-security bullseye-security/main amd64 krb5-multidev amd64 1.18.3-6+deb11u6 [158 kB]\r\n",
      "Get:12 http://deb.debian.org/debian-security bullseye-security/main amd64 libkrb5-dev amd64 1.18.3-6+deb11u6 [47.9 kB]\r\n",
      "Fetched 1423 kB in 4s (383 kB/s)        \r\n",
      "debconf: delaying package configuration, since apt-utils is not installed\r\n",
      "(Reading database ... 19072 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libcom-err2_1.46.2-2+deb11u1_amd64.deb ...\r\n",
      "Unpacking libcom-err2:amd64 (1.46.2-2+deb11u1) over (1.46.2-2) ...\r\n",
      "Setting up libcom-err2:amd64 (1.46.2-2+deb11u1) ...\r\n",
      "(Reading database ... 19072 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libk5crypto3_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libk5crypto3:amd64 (1.18.3-6+deb11u6) over (1.18.3-6+deb11u4) ...\r\n",
      "Setting up libk5crypto3:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "(Reading database ... 19072 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libkrb5support0_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libkrb5support0:amd64 (1.18.3-6+deb11u6) over (1.18.3-6+deb11u4) ...\r\n",
      "Setting up libkrb5support0:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "(Reading database ... 19072 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libkrb5-3_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libkrb5-3:amd64 (1.18.3-6+deb11u6) over (1.18.3-6+deb11u4) ...\r\n",
      "Setting up libkrb5-3:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "(Reading database ... 19072 files and directories currently installed.)\r\n",
      "Preparing to unpack .../libgssapi-krb5-2_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libgssapi-krb5-2:amd64 (1.18.3-6+deb11u6) over (1.18.3-6+deb11u4) ...\r\n",
      "Setting up libgssapi-krb5-2:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Selecting previously unselected package comerr-dev:amd64.\r\n",
      "(Reading database ... 19072 files and directories currently installed.)\r\n",
      "Preparing to unpack .../0-comerr-dev_2.1-1.46.2-2+deb11u1_amd64.deb ...\r\n",
      "Unpacking comerr-dev:amd64 (2.1-1.46.2-2+deb11u1) ...\r\n",
      "Selecting previously unselected package libgssrpc4:amd64.\r\n",
      "Preparing to unpack .../1-libgssrpc4_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libgssrpc4:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Selecting previously unselected package libkdb5-10:amd64.\r\n",
      "Preparing to unpack .../2-libkdb5-10_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libkdb5-10:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Selecting previously unselected package libkadm5srv-mit12:amd64.\r\n",
      "Preparing to unpack .../3-libkadm5srv-mit12_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libkadm5srv-mit12:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Selecting previously unselected package libkadm5clnt-mit12:amd64.\r\n",
      "Preparing to unpack .../4-libkadm5clnt-mit12_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libkadm5clnt-mit12:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Selecting previously unselected package krb5-multidev:amd64.\r\n",
      "Preparing to unpack .../5-krb5-multidev_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking krb5-multidev:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Selecting previously unselected package libkrb5-dev:amd64.\r\n",
      "Preparing to unpack .../6-libkrb5-dev_1.18.3-6+deb11u6_amd64.deb ...\r\n",
      "Unpacking libkrb5-dev:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Setting up libgssrpc4:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Setting up comerr-dev:amd64 (2.1-1.46.2-2+deb11u1) ...\r\n",
      "Setting up libkadm5clnt-mit12:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Setting up libkdb5-10:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Setting up libkadm5srv-mit12:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Setting up krb5-multidev:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Setting up libkrb5-dev:amd64 (1.18.3-6+deb11u6) ...\r\n",
      "Processing triggers for libc-bin (2.31-13+deb11u8) ...\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (2.32.3)\r\n",
      "Collecting requests-kerberos\r\n",
      "  Downloading requests_kerberos-0.15.0-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests) (1.26.20)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests) (2025.1.31)\r\n",
      "Collecting cryptography>=1.3 (from requests-kerberos)\r\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)\r\n",
      "Collecting pyspnego[kerberos] (from requests-kerberos)\r\n",
      "  Downloading pyspnego-0.11.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/site-packages (from cryptography>=1.3->requests-kerberos) (1.17.1)\r\n",
      "Collecting gssapi>=1.6.0 (from pyspnego[kerberos]->requests-kerberos)\r\n",
      "  Downloading gssapi-1.9.0.tar.gz (94 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m94.3/94.3 kB\u001B[0m \u001B[31m865.0 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting krb5>=0.3.0 (from pyspnego[kerberos]->requests-kerberos)\r\n",
      "  Downloading krb5-0.7.1.tar.gz (235 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m235.7/235.7 kB\u001B[0m \u001B[31m2.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=1.3->requests-kerberos) (2.22)\r\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.11/site-packages (from gssapi>=1.6.0->pyspnego[kerberos]->requests-kerberos) (5.2.1)\r\n",
      "Downloading requests_kerberos-0.15.0-py2.py3-none-any.whl (12 kB)\r\n",
      "Downloading cryptography-44.0.2-cp39-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.2/4.2 MB\u001B[0m \u001B[31m15.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading pyspnego-0.11.2-py3-none-any.whl (130 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m130.5/130.5 kB\u001B[0m \u001B[31m13.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hBuilding wheels for collected packages: gssapi, krb5\r\n",
      "  Building wheel for gssapi (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for gssapi: filename=gssapi-1.9.0-cp311-cp311-linux_x86_64.whl size=4947919 sha256=4d126840e65036c2fc85d255568d270899a8c5df40ec346cf83a8a36af5b486e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/9d/79/8b/6300aa69684280a0de217fd573097478c724cb0ce75e9fab73\r\n",
      "  Building wheel for krb5 (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for krb5: filename=krb5-0.7.1-cp311-cp311-linux_x86_64.whl size=7880060 sha256=90950a3d88c686bc96d0b1535a643ef60fcdc6152c487db2cf4f2664d91b52d8\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/4c/fe/e39ec81c33db263e0e0d963af4c93b564f5deeedef93e4bbdb\r\n",
      "Successfully built gssapi krb5\r\n",
      "Installing collected packages: krb5, gssapi, cryptography, pyspnego, requests-kerberos\r\n",
      "Successfully installed cryptography-44.0.2 gssapi-1.9.0 krb5-0.7.1 pyspnego-0.11.2 requests-kerberos-0.15.0\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:21:35.107551Z",
     "start_time": "2025-03-10T11:21:11.234781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import requests\n",
    "from requests_kerberos import HTTPKerberosAuth, OPTIONAL\n",
    "\n",
    "# Настройка Kerberos‑аутентификации для HTTP\n",
    "kerberos_auth = HTTPKerberosAuth(mutual_authentication=OPTIONAL)\n",
    "\n",
    "# URL HBase REST сервера\n",
    "BASE_URL = \"http://hbase-rest:8080\"\n",
    "\n",
    "def b64(s: str) -> str:\n",
    "    \"\"\"Преобразует строку в base64.\"\"\"\n",
    "    return base64.b64encode(s.encode()).decode()\n",
    "\n",
    "def check_table_exists(table_name: str) -> bool:\n",
    "    url = f\"{BASE_URL}/{table_name}/schema\"\n",
    "    response = requests.get(url, auth=kerberos_auth)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Таблица {table_name} существует.\")\n",
    "        return True\n",
    "    elif response.status_code == 404:\n",
    "        print(f\"Таблица {table_name} не найдена.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Ошибка проверки таблицы {table_name}: {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "def create_table(table_name: str):\n",
    "    # XML-схема таблицы. Здесь создается одна колонка \"cf\".\n",
    "    schema_xml = f\"\"\"<TableSchema name=\"{table_name}\">\n",
    "  <ColumnSchema name=\"cf\" />\n",
    "</TableSchema>\"\"\"\n",
    "    url = f\"{BASE_URL}/{table_name}/schema\"\n",
    "    headers = {'Content-Type': 'text/xml'}\n",
    "    response = requests.post(url, data=schema_xml, headers=headers, auth=kerberos_auth)\n",
    "    if response.status_code == 201:\n",
    "        print(f\"Таблица {table_name} успешно создана.\")\n",
    "    else:\n",
    "        print(f\"Не удалось создать таблицу {table_name}. Статус: {response.status_code}. Ответ: {response.text}\")\n",
    "\n",
    "def insert_row(table_name: str, row_key: str, data: dict):\n",
    "    \"\"\"\n",
    "    Вставка строки в таблицу через HBase REST API.\n",
    "    Формируем XML, где ключ и значения кодируются в base64.\n",
    "    \"\"\"\n",
    "    cells = \"\"\n",
    "    for col, val in data.items():\n",
    "        column = b64(\"cf:\" + col)\n",
    "        value = b64(val)\n",
    "        cells += f'<Cell column=\"{column}\">{value}</Cell>\\n'\n",
    "    cellset_xml = f\"\"\"<CellSet>\n",
    "  <Row key=\"{b64(row_key)}\">\n",
    "    {cells.strip()}\n",
    "  </Row>\n",
    "</CellSet>\"\"\"\n",
    "    url = f\"{BASE_URL}/{table_name}/{row_key}\"\n",
    "    headers = {'Content-Type': 'text/xml'}\n",
    "    response = requests.put(url, data=cellset_xml, headers=headers, auth=kerberos_auth)\n",
    "    if response.status_code in [200, 201]:\n",
    "        print(f\"Строка {row_key} вставлена в таблицу {table_name}.\")\n",
    "    else:\n",
    "        print(f\"Ошибка вставки строки {row_key}: {response.status_code}. Ответ: {response.text}\")\n",
    "\n",
    "def get_row(table_name: str, row_key: str):\n",
    "    url = f\"{BASE_URL}/{table_name}/{row_key}\"\n",
    "    headers = {'Accept': 'text/xml'}\n",
    "    response = requests.get(url, headers=headers, auth=kerberos_auth)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Данные строки {row_key}:\")\n",
    "        print(response.text)\n",
    "    else:\n",
    "        print(f\"Ошибка получения строки {row_key}: {response.status_code}. Ответ: {response.text}\")\n",
    "\n",
    "def delete_row(table_name: str, row_key: str):\n",
    "    url = f\"{BASE_URL}/{table_name}/{row_key}\"\n",
    "    response = requests.delete(url, auth=kerberos_auth)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Строка {row_key} удалена из таблицы {table_name}.\")\n",
    "    else:\n",
    "        print(f\"Ошибка удаления строки {row_key}: {response.status_code}. Ответ: {response.text}\")\n",
    "\n",
    "def main():\n",
    "    table1 = \"table1\"\n",
    "    table2 = \"table2\"\n",
    "\n",
    "    # Создание таблиц, если они отсутствуют\n",
    "    if not check_table_exists(table1):\n",
    "        create_table(table1)\n",
    "    if not check_table_exists(table2):\n",
    "        create_table(table2)\n",
    "\n",
    "    # Пример CRUD операций на таблице table1\n",
    "    row_key = \"row1\"\n",
    "    data = {\"col1\": \"value1\", \"col2\": \"value2\"}\n",
    "\n",
    "    print(\"\\nВставка строки:\")\n",
    "    insert_row(table1, row_key, data)\n",
    "\n",
    "    print(\"\\nПолучение строки:\")\n",
    "    get_row(table1, row_key)\n",
    "\n",
    "    print(\"\\nУдаление строки:\")\n",
    "    delete_row(table1, row_key)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "f993d48843abc61d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Таблица table1 не найдена.\n",
      "Таблица table1 успешно создана.\n",
      "Таблица table2 не найдена.\n",
      "Таблица table2 успешно создана.\n",
      "\n",
      "Вставка строки:\n",
      "Строка row1 вставлена в таблицу table1.\n",
      "\n",
      "Получение строки:\n",
      "Данные строки row1:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?><CellSet><Row key=\"cm93MQ==\"><Cell column=\"Y2Y6Y29sMQ==\" timestamp=\"1741605695067\">dmFsdWUx</Cell><Cell column=\"Y2Y6Y29sMg==\" timestamp=\"1741605695067\">dmFsdWUy</Cell></Row></CellSet>\n",
      "\n",
      "Удаление строки:\n",
      "Строка row1 удалена из таблицы table1.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:22:01.452654Z",
     "start_time": "2025-03-10T11:22:00.450498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!apt-get install curl -y\n",
    "!curl http://hbase-rest:8080/\n"
   ],
   "id": "625f9f6d06e8eade",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\r\n",
      "Building dependency tree... Done\r\n",
      "Reading state information... Done\r\n",
      "curl is already the newest version (7.74.0-1.3+deb11u14).\r\n",
      "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\r\n",
      "table1\r\n",
      "table2\r\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-10T11:28:22.821294Z",
     "start_time": "2025-03-10T11:28:20.233015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "def encode_str(s):\n",
    "    \"\"\"Кодирует строку в Base64.\"\"\"\n",
    "    return base64.b64encode(s.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "def check_table_exists(base_url, table):\n",
    "    url = f\"{base_url}/{table}/schema\"\n",
    "    headers = {\"Accept\": \"application/xml\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Таблица '{table}' существует.\")\n",
    "        return True\n",
    "    elif response.status_code == 404:\n",
    "        print(f\"Таблица '{table}' не найдена.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Ошибка при проверке таблицы '{table}': {response.status_code}\")\n",
    "        return False\n",
    "\n",
    "def create_table(base_url, table):\n",
    "    url = f\"{base_url}/{table}/schema\"\n",
    "    schema_xml = f\"\"\"<?xml version=\"1.0\"?>\n",
    "<TableSchema name=\"{table}\">\n",
    "  <ColumnSchema name=\"cf\"/>\n",
    "</TableSchema>\"\"\"\n",
    "    headers = {\"Content-Type\": \"text/xml\"}\n",
    "    response = requests.post(url, data=schema_xml, headers=headers)\n",
    "    if response.status_code in (200, 201):\n",
    "        print(f\"Таблица '{table}' успешно создана.\")\n",
    "    else:\n",
    "        print(f\"Не удалось создать таблицу '{table}': {response.status_code}, {response.text}\")\n",
    "\n",
    "def insert_row(base_url, table, row_key, data):\n",
    "    \"\"\"\n",
    "    Вставляет строку в таблицу.\n",
    "    data – словарь вида { \"column\": \"value\", ... }.\n",
    "    В этом примере колонка формируется как 'cf:column'.\n",
    "    \"\"\"\n",
    "    row_encoded = encode_str(row_key)\n",
    "    cells = \"\"\n",
    "    for col, val in data.items():\n",
    "        col_full = f\"cf:{col}\"\n",
    "        cells += f'<Cell column=\"{encode_str(col_full)}\">{encode_str(val)}</Cell>'\n",
    "    cellset_xml = f\"\"\"<?xml version=\"1.0\"?>\n",
    "<CellSet>\n",
    "  <Row key=\"{row_encoded}\">\n",
    "    {cells}\n",
    "  </Row>\n",
    "</CellSet>\"\"\"\n",
    "    url = f\"{base_url}/{table}/{row_key}\"\n",
    "    headers = {\"Content-Type\": \"text/xml\"}\n",
    "    response = requests.put(url, data=cellset_xml, headers=headers)\n",
    "    if response.status_code in (200, 201):\n",
    "        print(f\"Строка '{row_key}' вставлена в таблицу '{table}'.\")\n",
    "    else:\n",
    "        print(f\"Ошибка вставки строки '{row_key}' в таблицу '{table}': {response.status_code}, {response.text}\")\n",
    "\n",
    "def get_row(base_url, table, row_key):\n",
    "    url = f\"{base_url}/{table}/{row_key}\"\n",
    "    headers = {\"Accept\": \"text/xml\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Строка '{row_key}' получена:\")\n",
    "        print(response.text)\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Ошибка получения строки '{row_key}' из таблицы '{table}': {response.status_code}, {response.text}\")\n",
    "        return None\n",
    "\n",
    "def update_row(base_url, table, row_key, data):\n",
    "    # Обновление строки производится путем повторного вызова insert_row,\n",
    "    # так как HBase REST перезаписывает данные для указанного row_key.\n",
    "    print(f\"Обновление строки '{row_key}' в таблице '{table}'...\")\n",
    "    insert_row(base_url, table, row_key, data)\n",
    "\n",
    "def delete_row(base_url, table, row_key):\n",
    "    url = f\"{base_url}/{table}/{row_key}\"\n",
    "    response = requests.delete(url)\n",
    "    if response.status_code == 200:\n",
    "        print(f\"Строка '{row_key}' удалена из таблицы '{table}'.\")\n",
    "    else:\n",
    "        print(f\"Ошибка удаления строки '{row_key}' из таблицы '{table}': {response.status_code}, {response.text}\")\n",
    "\n",
    "def main():\n",
    "    base_url = \"http://hbase-rest:8080\"  # Адрес REST-сервера HBase\n",
    "    table1 = \"table1\"\n",
    "    table2 = \"table2\"\n",
    "\n",
    "    # Проверяем наличие таблиц, создаем, если отсутствуют\n",
    "    if not check_table_exists(base_url, table1):\n",
    "        create_table(base_url, table1)\n",
    "    if not check_table_exists(base_url, table2):\n",
    "        create_table(base_url, table2)\n",
    "\n",
    "    # Пример операций для таблицы table1\n",
    "    row_key = \"row1\"\n",
    "    initial_data = {\"col1\": \"value1\", \"col2\": \"value2\"}\n",
    "    updated_data = {\"col1\": \"new_value1\", \"col3\": \"value3\"}\n",
    "\n",
    "    # Вставка строки\n",
    "    insert_row(base_url, table1, row_key, initial_data)\n",
    "\n",
    "    # Получение строки\n",
    "    get_row(base_url, table1, row_key)\n",
    "\n",
    "    # Обновление строки\n",
    "    update_row(base_url, table1, row_key, updated_data)\n",
    "\n",
    "    # Получение обновленной строки\n",
    "    get_row(base_url, table1, row_key)\n",
    "\n",
    "    # Удаление строки\n",
    "    delete_row(base_url, table1, row_key)\n",
    "\n",
    "    # Попытка получения удаленной строки\n",
    "    get_row(base_url, table1, row_key)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "b5179b9a185df647",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка при проверке таблицы 'table1': 406\n",
      "Таблица 'table1' успешно создана.\n",
      "Ошибка при проверке таблицы 'table2': 406\n",
      "Таблица 'table2' успешно создана.\n",
      "Строка 'row1' вставлена в таблицу 'table1'.\n",
      "Строка 'row1' получена:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?><CellSet><Row key=\"cm93MQ==\"><Cell column=\"Y2Y6Y29sMQ==\" timestamp=\"1741606102798\">dmFsdWUx</Cell><Cell column=\"Y2Y6Y29sMg==\" timestamp=\"1741606102798\">dmFsdWUy</Cell></Row></CellSet>\n",
      "Обновление строки 'row1' в таблице 'table1'...\n",
      "Строка 'row1' вставлена в таблицу 'table1'.\n",
      "Строка 'row1' получена:\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?><CellSet><Row key=\"cm93MQ==\"><Cell column=\"Y2Y6Y29sMQ==\" timestamp=\"1741606102808\">bmV3X3ZhbHVlMQ==</Cell><Cell column=\"Y2Y6Y29sMg==\" timestamp=\"1741606102798\">dmFsdWUy</Cell><Cell column=\"Y2Y6Y29sMw==\" timestamp=\"1741606102808\">dmFsdWUz</Cell></Row></CellSet>\n",
      "Строка 'row1' удалена из таблицы 'table1'.\n",
      "Ошибка получения строки 'row1' из таблицы 'table1': 404, Not found\r\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8d87c2eb608bddb4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
