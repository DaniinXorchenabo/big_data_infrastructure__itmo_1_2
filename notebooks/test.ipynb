{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:54:29.103471Z",
     "start_time": "2025-02-26T12:54:26.937315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import uuid\n",
    "\n",
    "from src.code.code.init_neural import device\n",
    "!pip install kagglehub[pandas-datasets]"
   ],
   "id": "705e6d5acc612d0c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub[pandas-datasets] in /usr/local/lib/python3.11/site-packages (0.3.10)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (24.2)\r\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (4.67.1)\r\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/site-packages (from kagglehub[pandas-datasets]) (2.2.3)\r\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas->kagglehub[pandas-datasets]) (1.26.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas->kagglehub[pandas-datasets]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas->kagglehub[pandas-datasets]) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas->kagglehub[pandas-datasets]) (2025.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->kagglehub[pandas-datasets]) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->kagglehub[pandas-datasets]) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->kagglehub[pandas-datasets]) (1.26.20)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->kagglehub[pandas-datasets]) (2025.1.31)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->kagglehub[pandas-datasets]) (1.17.0)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-27T14:02:44.211268Z",
     "start_time": "2025-02-27T14:02:43.165700Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import os\n",
    "ROOT_DIR = '/workspace/NN'\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "import shutil\n",
    "import kagglehub\n",
    "import torch\n",
    "\n",
    "dataset_path = os.path.join(ROOT_DIR, 'neural', 'datasets', 'fashionmnist')\n",
    "logs_path = os.path.join(ROOT_DIR, 'neural', 'logs')\n",
    "csv_file = os.path.join(dataset_path, 'fashion-mnist_train.csv')\n",
    "prod_weights_path = os.path.join(ROOT_DIR, 'neural', 'weights', 'prod')\n",
    "test_weights_path = os.path.join(ROOT_DIR, 'neural', 'weights', 'lab_1')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:02:44.940265Z",
     "start_time": "2025-02-27T14:02:44.928172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if not os.path.isdir(dataset_path):\n",
    "    source_dir = kagglehub.dataset_download(\"zalando-research/fashionmnist\")\n",
    "    target_dir = dataset_path\n",
    "    shutil.move(source_dir, target_dir)"
   ],
   "id": "932879f1c983e01e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T12:59:32.968779Z",
     "start_time": "2025-02-26T12:59:31.451948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(csv_file)\n",
    "data"
   ],
   "id": "6b23761a8eb4a2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0          2       0       0       0       0       0       0       0       0   \n",
       "1          9       0       0       0       0       0       0       0       0   \n",
       "2          6       0       0       0       0       0       0       0       5   \n",
       "3          0       0       0       0       1       2       0       0       0   \n",
       "4          3       0       0       0       0       0       0       0       0   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "59995      9       0       0       0       0       0       0       0       0   \n",
       "59996      1       0       0       0       0       0       0       0       0   \n",
       "59997      8       0       0       0       0       0       0       0       0   \n",
       "59998      8       0       0       0       0       0       0       0       0   \n",
       "59999      7       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0           0  ...         0         0         0         0         0   \n",
       "1           0  ...         0         0         0         0         0   \n",
       "2           0  ...         0         0         0        30        43   \n",
       "3           0  ...         3         0         0         0         0   \n",
       "4           0  ...         0         0         0         0         0   \n",
       "...       ...  ...       ...       ...       ...       ...       ...   \n",
       "59995       0  ...         0         0         0         0         0   \n",
       "59996       0  ...        73         0         0         0         0   \n",
       "59997       0  ...       160       162       163       135        94   \n",
       "59998       0  ...         0         0         0         0         0   \n",
       "59999       0  ...         0         0         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0             0         0         0         0         0  \n",
       "1             0         0         0         0         0  \n",
       "2             0         0         0         0         0  \n",
       "3             1         0         0         0         0  \n",
       "4             0         0         0         0         0  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "59995         0         0         0         0         0  \n",
       "59996         0         0         0         0         0  \n",
       "59997         0         0         0         0         0  \n",
       "59998         0         0         0         0         0  \n",
       "59999         0         0         0         0         0  \n",
       "\n",
       "[60000 rows x 785 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59995</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>160</td>\n",
       "      <td>162</td>\n",
       "      <td>163</td>\n",
       "      <td>135</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59998</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59999</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>60000 rows × 785 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T13:03:19.763614Z",
     "start_time": "2025-02-26T13:03:18.207386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class FashionMNISTDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (str): Путь к CSV-файлу с данными.\n",
    "            transform (callable, optional): Трансформации для изображения.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Получаем строку с данными\n",
    "        sample = self.data.iloc[idx]\n",
    "\n",
    "        # Предполагается, что в CSV первая колонка — метка,\n",
    "        # а остальные 28*28 пикселей записаны последовательно.\n",
    "        label = int(sample.iloc[0])\n",
    "        img_array = sample.iloc[1:].values.astype(np.uint8).reshape(28, 28)\n",
    "\n",
    "        # Если трансформации заданы, применяем их.\n",
    "        if self.transform:\n",
    "            img = self.transform(img_array)\n",
    "        else:\n",
    "            # Преобразуем в тензор, добавляя размер канала (1, 28, 28)\n",
    "            img = torch.tensor(img_array, dtype=torch.float32).unsqueeze(0) / 255.0\n",
    "\n",
    "        return img, label\n",
    "\n",
    "'''\n",
    "0 Футболка/топ\n",
    "1 Брюки\n",
    "2 Пуловер\n",
    "3 Платье\n",
    "4 Слой\n",
    "5 Сандалий\n",
    "6 Рубашка\n",
    "7 Кроссовок\n",
    "8 Мешков\n",
    "9 Ботинок на щиколотке\n",
    "'''\n",
    "\n",
    "# Пример использования:\n",
    "if __name__ == \"__main__\":\n",
    "    # Определяем трансформации: конвертация numpy массива в PIL Image и затем в тензор\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # Путь к CSV-файлу (отредактируй путь в соответствии с расположением файла)\n",
    "    train_csv = csv_file\n",
    "\n",
    "    # Создаём объект датасета\n",
    "    train_dataset = FashionMNISTDataset(csv_file=train_csv, transform=transform)\n",
    "\n",
    "    # Создаём DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Пример итерации по DataLoader'у\n",
    "    for images, labels in train_loader:\n",
    "        print(f\"Batch images shape: {images.shape}, labels shape: {labels.shape}\")\n",
    "        # Здесь можно добавить код для обучения модели\n",
    "        break\n"
   ],
   "id": "8108692ba62791db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([64, 1, 28, 28]), labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:02:52.095655Z",
     "start_time": "2025-02-27T14:02:50.091325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import FashionMNIST\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# Определяем лёгкую сверточную сеть для FashionMNIST\n",
    "class LightweightFashionMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LightweightFashionMNIST, self).__init__()\n",
    "        # Входное изображение: 1 x 28 x 28\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)   # -> 16 x 28 x 28\n",
    "        self.pool  = nn.MaxPool2d(2, 2)                             # -> 16 x 14 x 14\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)    # -> 32 x 14 x 14\n",
    "        # После второго pooling: 32 x 7 x 7\n",
    "        self.fc1   = nn.Linear(32 * 7 * 7, 64)\n",
    "        self.fc2   = nn.Linear(64, 10)  # 10 классов\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Определяем LightningModule\n",
    "class FashionMNISTLitModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3):\n",
    "        super(FashionMNISTLitModel, self).__init__()\n",
    "        self.model = LightweightFashionMNIST()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Метрики для обучения и валидации\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=10)\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='multiclass',num_classes=10)\n",
    "        self.val_precision = torchmetrics.Precision(task='multiclass',num_classes=10, average='macro')\n",
    "        self.val_recall = torchmetrics.Recall(task='multiclass', num_classes=10, average='macro')\n",
    "        self.val_auroc = torchmetrics.AUROC(task='multiclass',num_classes=10)\n",
    "        self.confmat = torchmetrics.ConfusionMatrix(task='multiclass', num_classes=10)\n",
    "        self.val_f1 = MulticlassF1Score( num_classes=10, average='macro')  # Добавлена метрика F1\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_accuracy.update(preds, y)\n",
    "        self.log('train/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    # Вместо training_epoch_end, используем on_train_epoch_end (без аргументов)\n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        self.log('train/epoch_time', epoch_time)\n",
    "        acc = self.train_accuracy.compute()\n",
    "        self.log('train/accuracy', acc, prog_bar=True)\n",
    "        self.train_accuracy.reset()\n",
    "        self.log('train/lr', self.learning_rate)\n",
    "\n",
    "    def on_train_epoch_start(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.val_accuracy.update(preds, y)\n",
    "        self.val_precision.update(preds, y)\n",
    "        self.val_recall.update(preds, y)\n",
    "        self.val_auroc.update(F.softmax(logits, dim=1), y)\n",
    "        self.val_f1.update(preds, y)  # Обновление F1 метрики\n",
    "\n",
    "        self.confmat.update(preds, y)\n",
    "        self.log('val/loss', loss, prog_bar=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        acc = self.val_accuracy.compute()\n",
    "        prec = self.val_precision.compute()\n",
    "        rec = self.val_recall.compute()\n",
    "        f1 = self.val_f1.compute()  # Вычисление F1 метрики\n",
    "        auroc = self.val_auroc.compute()\n",
    "        self.log('val/accuracy', acc, prog_bar=True)\n",
    "        self.log('val/precision', prec)\n",
    "        self.log('val/recall', rec)\n",
    "        self.log('val/auroc', auroc)\n",
    "        self.log('val/f1', f1)  # Логирование F1 метрики\n",
    "\n",
    "        # Логируем матрицу ошибок как изображение в TensorBoard\n",
    "        confmat = self.confmat.compute().cpu().numpy()\n",
    "        fig = self.plot_confusion_matrix(confmat)\n",
    "        self.logger.experiment.add_figure(\"Confusion Matrix\", fig, self.current_epoch)\n",
    "\n",
    "        self.val_accuracy.reset()\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.val_auroc.reset()\n",
    "        self.val_f1.reset()  # Сброс F1 метрики\n",
    "        self.confmat.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def plot_confusion_matrix(self, confmat):\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "        im = ax.imshow(confmat, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        ax.set(xticks=np.arange(confmat.shape[1]),\n",
    "               yticks=np.arange(confmat.shape[0]),\n",
    "               xticklabels=np.arange(10),\n",
    "               yticklabels=np.arange(10),\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label',\n",
    "               title='Confusion Matrix')\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        thresh = confmat.max() / 2.\n",
    "        for i in range(confmat.shape[0]):\n",
    "            for j in range(confmat.shape[1]):\n",
    "                ax.text(j, i, format(confmat[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if confmat[i, j] > thresh else \"black\")\n",
    "        fig.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Подготовка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = FashionMNIST(root=dataset_path, train=True, download=True, transform=transform)\n",
    "val_dataset   = FashionMNIST(root=dataset_path, train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "d6dc0e65c5043d0c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# Инициализируем модель\n",
    "learn_id = str(uuid.uuid4())\n",
    "model_name = 'fashion_MNIST_lite'\n",
    "model = FashionMNISTLitModel(learning_rate=1e-3)\n",
    "# Настраиваем TensorBoard логгер\n",
    "logger = TensorBoardLogger(\n",
    "        logs_path,\n",
    "        name=model_name,\n",
    "        version=learn_id,\n",
    "        sub_dir=(_sd:=f\"{(u_:=datetime.utcnow().strftime('%Y_%m_%d__%H_%M_%S'))}\"),\n",
    ")\n",
    "# Настраиваем колбэк для сохранения модели\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=(_tg_m:='val/auroc'),       # Метрика для мониторинга\n",
    "    dirpath=test_weights_path, # Каталог для сохранения моделей\n",
    "    filename=f'{model_name}_{_sd}_{learn_id}' + '-{epoch:02d}-{' + _tg_m + ':.2f}', # Имя файла\n",
    "    save_top_k=1,             # Сохранять только лучшую модель\n",
    "    mode='max'                # Минимизировать monitored метрику\n",
    ")\n",
    "\n",
    "# Создаем тренер PyTorch Lightning\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=50,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "# Запускаем обучение\n",
    "trainer.fit(model, train_loader, val_loader)"
   ],
   "id": "4505642880511b87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T11:42:01.458545Z",
     "start_time": "2025-02-27T11:42:01.335382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_model_path = checkpoint_callback.best_model_path\n",
    "\n",
    "target_path_to_model = os.path.join(prod_weights_path, os.path.split(os.path.split(best_model_path)[0])[1] + '_' + os.path.split(best_model_path)[1])\n",
    "\n",
    "shutil.move(best_model_path, target_path_to_model)\n"
   ],
   "id": "ad0d59906d598353",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/workspace/NN/my_checkpoints/fashion_MNIST_lite_2025_02_27__11_35_34_af5d8943-4cf8-4204-b7a3-f39e5a49e673-epoch=10-val/auroc=0.99.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m/usr/local/lib/python3.11/shutil.py:853\u001B[0m, in \u001B[0;36mmove\u001B[0;34m(src, dst, copy_function)\u001B[0m\n\u001B[1;32m    852\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 853\u001B[0m     os\u001B[38;5;241m.\u001B[39mrename(src, real_dst)\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/workspace/NN/my_checkpoints/fashion_MNIST_lite_2025_02_27__11_35_34_af5d8943-4cf8-4204-b7a3-f39e5a49e673-epoch=10-val/auroc=0.99.ckpt' -> '/workspace/NN/neural/prod'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m best_model_path \u001B[38;5;241m=\u001B[39m checkpoint_callback\u001B[38;5;241m.\u001B[39mbest_model_path\n\u001B[0;32m----> 2\u001B[0m \u001B[43mshutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmove\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbest_model_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprod_weights_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m path_to_model \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(prod_weights_path, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39msplit(best_model_path)[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m      4\u001B[0m trained_model \u001B[38;5;241m=\u001B[39m FashionMNISTLitModel\u001B[38;5;241m.\u001B[39mload_from_checkpoint(\n\u001B[1;32m      5\u001B[0m     checkpoint_path\u001B[38;5;241m=\u001B[39mpath_to_model,\n\u001B[1;32m      6\u001B[0m     map_location\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(device\u001B[38;5;241m=\u001B[39mdevice)  \u001B[38;5;66;03m# или 'cuda' для GPU\u001B[39;00m\n\u001B[1;32m      7\u001B[0m )\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/shutil.py:873\u001B[0m, in \u001B[0;36mmove\u001B[0;34m(src, dst, copy_function)\u001B[0m\n\u001B[1;32m    871\u001B[0m         rmtree(src)\n\u001B[1;32m    872\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 873\u001B[0m         \u001B[43mcopy_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreal_dst\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    874\u001B[0m         os\u001B[38;5;241m.\u001B[39munlink(src)\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m real_dst\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/shutil.py:448\u001B[0m, in \u001B[0;36mcopy2\u001B[0;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[1;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misdir(dst):\n\u001B[1;32m    447\u001B[0m     dst \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dst, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(src))\n\u001B[0;32m--> 448\u001B[0m \u001B[43mcopyfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfollow_symlinks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_symlinks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    449\u001B[0m copystat(src, dst, follow_symlinks\u001B[38;5;241m=\u001B[39mfollow_symlinks)\n\u001B[1;32m    450\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m dst\n",
      "File \u001B[0;32m/usr/local/lib/python3.11/shutil.py:256\u001B[0m, in \u001B[0;36mcopyfile\u001B[0;34m(src, dst, follow_symlinks)\u001B[0m\n\u001B[1;32m    254\u001B[0m     os\u001B[38;5;241m.\u001B[39msymlink(os\u001B[38;5;241m.\u001B[39mreadlink(src), dst)\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(src, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fsrc:\n\u001B[1;32m    257\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    258\u001B[0m             \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(dst, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fdst:\n\u001B[1;32m    259\u001B[0m                 \u001B[38;5;66;03m# macOS\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/workspace/NN/my_checkpoints/fashion_MNIST_lite_2025_02_27__11_35_34_af5d8943-4cf8-4204-b7a3-f39e5a49e673-epoch=10-val/auroc=0.99.ckpt'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:07:07.939354Z",
     "start_time": "2025-02-27T14:07:07.868894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_path_to_model = '/workspace/NN/neural/weights/prod/fashion_MNIST_lite_2025_02_27__11_35_34_af5d8943-4cf8-4204-b7a3-f39e5a49e673-epoch=10-val_auroc=0.99.ckpt'\n",
    "trained_model = FashionMNISTLitModel.load_from_checkpoint(\n",
    "    checkpoint_path=target_path_to_model,\n",
    "    map_location=torch.device(device=device)  # или 'cuda' для GPU\n",
    ")\n",
    "model_name='fashion_MNIST_lite'\n",
    "learn_id = 'af5d8943-4cf8-4204-b7a3-f39e5a49e673'\n",
    "torch.save(trained_model.model.state_dict(), os.path.join(os.path.split(target_path_to_model)[0], f'{model_name}_{learn_id}.pth'))"
   ],
   "id": "d0032a0d8dd89056",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T14:07:42.689170Z",
     "start_time": "2025-02-27T14:07:42.660558Z"
    }
   },
   "cell_type": "code",
   "source": "os.path.join(os.path.split(target_path_to_model)[0], f'{model_name}_{learn_id}.pth')\n",
   "id": "46df42f2c98392c0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/NN/neural/weights/prod/fashion_MNIST_lite_af5d8943-4cf8-4204-b7a3-f39e5a49e673.pth'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T13:44:31.260232Z",
     "start_time": "2025-02-28T13:44:29.190402Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install huggingface_hub\n",
   "id": "e0ef63cd84053a8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/site-packages (0.29.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (3.17.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (2024.12.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/site-packages (from huggingface_hub) (4.12.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub) (1.26.20)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests->huggingface_hub) (2025.1.31)\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T13:47:42.177992Z",
     "start_time": "2025-02-28T13:47:41.956268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.configs.config import CONFIG\n",
    "from huggingface_hub import hf_hub_download\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "if os.path.isfile(CONFIG.MODEL_DIR):\n",
    "    model_path = hf_hub_download(repo_id=CONFIG.AI_WEIGHTS_REPO, filename=CONFIG.AI_WEIGHTS_REPO_FILENAME)\n",
    "    source_dir = model_path\n",
    "    target_dir = CONFIG.MODEL_DIR\n",
    "    shutil.move(source_dir, target_dir)"
   ],
   "id": "a1dbef0b842bdc0e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f05a5ca7cc546dfc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
