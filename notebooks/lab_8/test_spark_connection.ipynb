{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Spark deploying",
   "id": "930bcbfb61c45a28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Without helm",
   "id": "7a699774d8167751"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!docker-compose -f .\\docker\\entrypoint.docker-compose.yml  -f .\\docker\\spark\\spark.docker-compose.yml --env-file=./env/.env  up producer -d --force-recreate\n",
    "\n",
    "!wsl -d <Distributive-Name>\n",
    "!kubectl create namespace spark\n",
    "!kubectl apply -f ./docker/kubernetes/deprecated/no-auth-spark-master.yaml -n spark\n",
    "!kubectl apply -f ./docker/kubernetes/deprecated/no-auth-spark-master.yaml -n spark"
   ],
   "id": "ac738fb865754e09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With helm",
   "id": "7b9094065038f4be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "docker-compose -f .\\docker\\entrypoint.docker-compose.yml  -f .\\docker\\spark\\spark.docker-compose.yml --env-file=./env/.env  up producer -d --force-recreate\n",
    "\n",
    "wsl -d <Distributive-Name>\n",
    "```\n",
    "\n",
    "If it's helm creating:\n",
    "```bash\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm search repo bitnami\n",
    "helm install kayvan-release daniinxorchenabo/spark-k8s:latest\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "helm upgrade kayvan-release bitnami/spark \\\n",
    "  --install \\\n",
    "  -f ./docker/kubernetes/helm/spark-config.yaml \\\n",
    "  --set global.security.allowInsecureImages=true \\\n",
    "  --set image.tag=latest \\\n",
    "  --set image.registry=\"docker.io\" \\\n",
    "  --set image.repository=\"daniinxorchenabo/spark-k8s\" \\\n",
    "  --set image.tag=\"latest\" \\\n",
    "  --set master.resources.requests.cpu=\"1\" \\\n",
    "  --set master.resources.limits.cpu=\"1\" \\\n",
    "  --set worker.resources.requests.cpu=\"8\" \\\n",
    "  --set worker.resources.limits.cpu=\"8\" \\\n",
    "  --set worker.coreLimit=\"8\" \\\n",
    "  --set worker.replicaCount=\"4\" \\\n",
    "  --force\n",
    "kubectl apply -f ./docker/kubernetes/helm/spark-master-service.yaml\n",
    "```\n",
    "\n",
    "Some diagnostic commands:\n",
    "```bash\n",
    "kubectl get svc -n default\n",
    "kubectl get pod -n default\n",
    "kubectl get nodes -o wide\n",
    "kubectl get pods --show-labels\n",
    "curl host.docker.internal:30080\n",
    "```"
   ],
   "id": "578e68bdc9a5c1e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connection tests into spark pods",
   "id": "c64e534dc91146e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correct:",
   "id": "7b07f03f74ce5551"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-20T19:04:14.904762Z",
     "start_time": "2025-06-20T19:04:07.395890Z"
    }
   },
   "source": [
    "import os\n",
    "os.environ[\"HOME\"] = \"/tmp\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/bitnami/python/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/opt/bitnami/python/bin/python3\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Создаём SparkSession, указываем master как spark://<имя-сервиса>:7077\n",
    "#    При работе внутри Pod’а Kubernetes автоматически резолвит \"spark-master\" в его ClusterIP.\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"standalone-simple\")\n",
    "        # .master(\"spark://kayvan-release-spark-master-0:7077\")\n",
    "        .master(\"spark://host.docker.internal:30077\")\n",
    "        .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "        .config(\"spark.executor.instances\", \"1\")      # сколько Executors запустить\n",
    "        .config(\"spark.executor.cores\", \"1\")          # по одному CPU‐ядру\n",
    "        .config(\"spark.executor.memory\", \"1g\")      # по 512 МБ памяти\n",
    "        .config(\"spark.driver.memory\", \"1g\")        # драйверу тоже ограничим RAM, если нужно\n",
    "        .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "        .config(\"spark.submit.deployMode\", \"client\")\n",
    "        .config(\"spark.driver.bindAddress\",  \"0.0.0.0\")\n",
    "        .config(\"spark.driver.port\",         \"45555\")\n",
    "        .config(\"spark.blockManager.port\",   \"45556\")\n",
    "\n",
    "\n",
    "    .config(\"spark.pyspark.python\", \"/opt/bitnami/python/bin/python3\")              # путь к python3 внутри контейнера\n",
    ".config(\"spark.executorEnv.PYSPARK_PYTHON\", \"/opt/bitnami/python/bin/python3\")  # тоже самое для среды executor’а\n",
    "\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2) Делаем простую проверку\n",
    "df = spark.range(1_000)                 # создаём DataFrame с числами от 0 до 999 999\n",
    "result = df.selectExpr(\"sum(id) as total\")  # суммируем колонку \"id\"\n",
    "result.show()                               # показываем в консоли: ожидаем 499999500000\n",
    "\n",
    "spark.stop()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/20 19:04:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| total|\n",
      "+------+\n",
      "|499500|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## test mongo",
   "id": "5dc46bdef1daea40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-20T19:20:00.438799Z",
     "start_time": "2025-06-20T19:16:39.628040Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"HOME\"] = \"/tmp\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/bitnami/python/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/opt/bitnami/python/bin/python3\"\n",
    "MONGO_USER = 'admin'  # os.environ[\"MONGO_USER\"]\n",
    "MONGO_PASS = 'password'  # os.environ[\"MONGO_PASSWORD\"]\n",
    "MONGO_HOST = \"host.docker.internal\"\n",
    "MONGO_PORT = \"30717\"\n",
    "MONGO_ADDR = f\"{MONGO_USER}:{MONGO_PASS}@{MONGO_HOST}:{MONGO_PORT}\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Создаём SparkSession, указываем master как spark://<имя-сервиса>:7077\n",
    "#    При работе внутри Pod’а Kubernetes автоматически резолвит \"spark-master\" в его ClusterIP.\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"DataMart-HTTP-Service\")\n",
    "        # .master(\"spark://kayvan-release-spark-master-0:7077\")\n",
    "        .master(\"spark://host.docker.internal:30077\")\n",
    "        .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "        .config(\"spark.executor.instances\", \"2\")      # сколько Executors запустить\n",
    "        .config(\"spark.executor.cores\", \"1\")          # по одному CPU‐ядру\n",
    "        .config(\"spark.executor.memory\", \"1g\")      # по 512 МБ памяти\n",
    "        # .config(\"spark.driver.memory\", \"1g\")        # драйверу тоже ограничим RAM, если нужно\n",
    "        #\n",
    "        # .config(\"spark.submit.deployMode\", \"client\")\n",
    "        # .config(\"spark.driver.bindAddress\",  \"0.0.0.0\")\n",
    "        # # .config(\"spark.driver.port\",         \"45555\")\n",
    "        # # .config(\"spark.blockManager.port\",   \"45556\")\n",
    "        # .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "        #\n",
    "        # .config(\"spark.driver.port\",         \"31555\")\n",
    "        # .config(\"spark.blockManager.port\",   \"31556\")\n",
    "        .config(\"spark.mongodb.read.connection.uri\", f\"mongodb://{MONGO_ADDR}\")\n",
    "        .config(\"spark.mongodb.write.connection.uri\", f\"mongodb://{MONGO_ADDR}\")\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0,\"\n",
    "                                       \"com.redislabs:spark-redis_2.12:3.1.0,\"\n",
    "                                       \"org.neo4j:neo4j-connector-apache-spark_2.12:5.3.1_for_spark_3\")\n",
    "        .config(\"spark.pyspark.python\", \"/opt/bitnami/python/bin/python3\")              # путь к python3 внутри контейнера\n",
    "        .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"/opt/bitnami/python/bin/python3\")  # тоже самое для среды executor’а\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2) Делаем простую проверку\n",
    "from pyspark.sql.types import (StructType, StructField,\n",
    "                               StringType, IntegerType, MapType,\n",
    "                               DoubleType\n",
    "                               )\n",
    "\n",
    "# Пример определения схемы. Настройте схему под структуру ваших данных.\n",
    "custom_schema = StructType([\n",
    "    StructField(\"_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "])\n",
    "\n",
    "df = spark.read.schema(custom_schema).format(\"mongodb\") \\\n",
    "  .options(host=f\"{MONGO_HOST}:{MONGO_PORT}\", database=\"off\", collection='products').load()  # , database=\"off\", collection='products'\n",
    "\n",
    "\n",
    "# Просмотр схемы и первых строк\n",
    "# print(df.count())\n",
    "df.printSchema()\n",
    "df.show(20)"
   ],
   "id": "b5c188e883e856e7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /tmp/.ivy2/cache\n",
      "The jars for the packages stored in: /tmp/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "com.redislabs#spark-redis_2.12 added as a dependency\n",
      "org.neo4j#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-d9bb9ce0-2ef8-4bf5-bdb9-79d20127c6d4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.3.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      "\tfound com.redislabs#spark-redis_2.12;3.1.0 in central\n",
      "\tfound redis.clients#jedis;3.9.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12;5.3.1_for_spark_3 in central\n",
      "\tfound org.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.1 in central\n",
      "\tfound org.neo4j.driver#neo4j-java-driver;4.4.17 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.4 in central\n",
      "\tfound org.apache.xbean#xbean-asm6-shaded;4.10 in central\n",
      "\tfound org.neo4j#neo4j-cypher-dsl;2022.9.2 in central\n",
      "\tfound org.apiguardian#apiguardian-api;1.1.2 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.3.0/mongo-spark-connector_2.12-10.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;10.3.0!mongo-spark-connector_2.12.jar (223ms)\n",
      "downloading https://repo1.maven.org/maven2/com/redislabs/spark-redis_2.12/3.1.0/spark-redis_2.12-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.redislabs#spark-redis_2.12;3.1.0!spark-redis_2.12.jar (199ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/neo4j-connector-apache-spark_2.12/5.3.1_for_spark_3/neo4j-connector-apache-spark_2.12-5.3.1_for_spark_3.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#neo4j-connector-apache-spark_2.12;5.3.1_for_spark_3!neo4j-connector-apache-spark_2.12.jar (498ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.8.2/mongodb-driver-sync-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.8.2!mongodb-driver-sync.jar (137ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.8.2/bson-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.8.2!bson.jar (231ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.8.2/mongodb-driver-core-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.8.2!mongodb-driver-core.jar (355ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/4.8.2/bson-record-codec-4.8.2.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson-record-codec;4.8.2!bson-record-codec.jar (132ms)\n",
      "downloading https://repo1.maven.org/maven2/redis/clients/jedis/3.9.0/jedis-3.9.0.jar ...\n",
      "\t[SUCCESSFUL ] redis.clients#jedis;3.9.0!jedis.jar (165ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/1.7.30/slf4j-api-1.7.30.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;1.7.30!slf4j-api.jar (130ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (132ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/neo4j-connector-apache-spark_2.12_common/5.3.1/neo4j-connector-apache-spark_2.12_common-5.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.1!neo4j-connector-apache-spark_2.12_common.jar (-331ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/driver/neo4j-java-driver/4.4.17/neo4j-java-driver-4.4.17.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j.driver#neo4j-java-driver;4.4.17!neo4j-java-driver.jar (-717ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/xbean/xbean-asm6-shaded/4.10/xbean-asm6-shaded-4.10.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.xbean#xbean-asm6-shaded;4.10!xbean-asm6-shaded.jar(bundle) (137ms)\n",
      "downloading https://repo1.maven.org/maven2/org/neo4j/neo4j-cypher-dsl/2022.9.2/neo4j-cypher-dsl-2022.9.2.jar ...\n",
      "\t[SUCCESSFUL ] org.neo4j#neo4j-cypher-dsl;2022.9.2!neo4j-cypher-dsl.jar (145ms)\n",
      "downloading https://repo1.maven.org/maven2/org/reactivestreams/reactive-streams/1.0.4/reactive-streams-1.0.4.jar ...\n",
      "\t[SUCCESSFUL ] org.reactivestreams#reactive-streams;1.0.4!reactive-streams.jar (129ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apiguardian/apiguardian-api/1.1.2/apiguardian-api-1.1.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apiguardian#apiguardian-api;1.1.2!apiguardian-api.jar (130ms)\n",
      ":: resolution report :: resolve 18183ms :: artifacts dl 1704ms\n",
      "\t:: modules in use:\n",
      "\tcom.redislabs#spark-redis_2.12;3.1.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.xbean#xbean-asm6-shaded;4.10 from central in [default]\n",
      "\torg.apiguardian#apiguardian-api;1.1.2 from central in [default]\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.3.0 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12;5.3.1_for_spark_3 from central in [default]\n",
      "\torg.neo4j#neo4j-connector-apache-spark_2.12_common;5.3.1 from central in [default]\n",
      "\torg.neo4j#neo4j-cypher-dsl;2022.9.2 from central in [default]\n",
      "\torg.neo4j.driver#neo4j-java-driver;4.4.17 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tredis.clients#jedis;3.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   16  |   16  |   0   ||   16  |   16  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-d9bb9ce0-2ef8-4bf5-bdb9-79d20127c6d4\n",
      "\tconfs: [default]\n",
      "\t16 artifacts copied, 0 already retrieved (22219kB/35ms)\n",
      "25/06/20 19:17:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/20 19:17:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:17:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:18:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:18:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:18:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:18:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:19:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:19:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:19:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/20 19:19:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=59>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/usr/local/lib/python3.12/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o35.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "25/06/20 19:20:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o59.showString",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 68\u001B[39m\n\u001B[32m     65\u001B[39m \u001B[38;5;66;03m# Просмотр схемы и первых строк\u001B[39;00m\n\u001B[32m     66\u001B[39m \u001B[38;5;66;03m# print(df.count())\u001B[39;00m\n\u001B[32m     67\u001B[39m df.printSchema()\n\u001B[32m---> \u001B[39m\u001B[32m68\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:947\u001B[39m, in \u001B[36mDataFrame.show\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    887\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m = \u001B[32m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] = \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    888\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001B[39;00m\n\u001B[32m    889\u001B[39m \n\u001B[32m    890\u001B[39m \u001B[33;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    945\u001B[39m \u001B[33;03m    name | Bob\u001B[39;00m\n\u001B[32m    946\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m947\u001B[39m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:965\u001B[39m, in \u001B[36mDataFrame._show_string\u001B[39m\u001B[34m(self, n, truncate, vertical)\u001B[39m\n\u001B[32m    959\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[32m    960\u001B[39m         error_class=\u001B[33m\"\u001B[39m\u001B[33mNOT_BOOL\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    961\u001B[39m         message_parameters={\u001B[33m\"\u001B[39m\u001B[33marg_name\u001B[39m\u001B[33m\"\u001B[39m: \u001B[33m\"\u001B[39m\u001B[33mvertical\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33marg_type\u001B[39m\u001B[33m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical).\u001B[34m__name__\u001B[39m},\n\u001B[32m    962\u001B[39m     )\n\u001B[32m    964\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[32m--> \u001B[39m\u001B[32m965\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    966\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    967\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdeco\u001B[39m(*a: Any, **kw: Any) -> Any:\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    180\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    181\u001B[39m         converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/py4j/protocol.py:334\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    330\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    331\u001B[39m                 \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    332\u001B[39m                 \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n\u001B[32m    333\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m334\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    335\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    336\u001B[39m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name))\n\u001B[32m    337\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    338\u001B[39m     \u001B[38;5;28mtype\u001B[39m = answer[\u001B[32m1\u001B[39m]\n",
      "\u001B[31mPy4JError\u001B[39m: An error occurred while calling o59.showString"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bad:",
   "id": "360367565100aa31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# driver_host = \"localhost\"\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join(ROOT_DIR, \"k8s-creds\", 'config')\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "# --conf spark.executor.memory=1G\n",
    "# --conf spark.executor.cores=1\n",
    "# pyspark-shell\n",
    "# \"\"\"\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"k8s-test-app\")\n",
    "     # .master(\"spark://localhost:7077\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    # .config(\"spark.driver.host\", \"localhost\")\n",
    "    .getOrCreate())\n",
    "\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "4383a4ad544c6ada"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ[\"HOME\"] = \"/tmp\"\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join('root', 'kube','config', 'config')\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join('k8s-creds', 'config')\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    SparkSession.builder.getOrCreate().stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"jupyter-on-spark\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    # .master(\"spark://https://kubernetes.default.svc:443\")\n",
    "    .config(\"spark.submit.deployMode\", \"client\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "    .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "    .getOrCreate()\n",
    ")\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()"
   ],
   "id": "f17fc4d9034b3b64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# import sys\n",
    "# from operator import add\n",
    "#\n",
    "# from pyspark.sql import SparkSession\n",
    "#\n",
    "#\n",
    "# spark = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .appName(\"PythonWordCount\")\\\n",
    "#      .master(\"spark://spark-master:7077\")\\\n",
    "#         .getOrCreate()\n",
    "#\n",
    "# lines = spark.read.text(\"Привет Привет привет\").rdd.map(lambda r: r[0])\n",
    "# counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "#                   .map(lambda x: (x, 1)) \\\n",
    "#                   .reduceByKey(add)\n",
    "# output = counts.collect()\n",
    "# for (word, count) in output:\n",
    "#     print(\"%s: %i\" % (word, count))\n",
    "#\n",
    "# spark.stop()"
   ],
   "id": "a5fb4a391a3eaa9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connection test into jupyter docker image",
   "id": "b9fb77dd26578dfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Correct:",
   "id": "3cf683df298c2b69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "ROOT_DIR = '/workspace/NN'\n",
    "os.chdir(ROOT_DIR)\n",
    "from pyspark.sql import SparkSession\n",
    "# driver_host = \"localhost\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join(ROOT_DIR, \"k8s-creds\", 'config')\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"k8s-test-app\")\n",
    "    # .master(\"k8s://https://kubernetes.docker.internal:6443\")\n",
    ".master(\"spark://host.docker.internal:30077\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"default\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "\n",
    "        .config(\"spark.executor.instances\", \"2\")      # сколько Executors запустить\n",
    "        .config(\"spark.executor.cores\", \"1\")          # по одному CPU‐ядру\n",
    "        .config(\"spark.executor.memory\", \"1g\")      # по 512 МБ памяти\n",
    "        .config(\"spark.driver.memory\", \"1g\")        # драйверу тоже ограничим RAM, если нужно\n",
    "\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.caCertFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"ca.crt\"))\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.clientKeyFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"client.key\"))\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.clientCertFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"client.crt\"))\n",
    "    # .config(\"spark.kubernetes.executor.podNamePrefix\", \"spark-exec\")\n",
    "\n",
    "    .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "    .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "\n",
    "    # .config(\"spark.submit.deployMode\", \"cluster\")\n",
    "\n",
    "    .config(\"spark.driver.bindAddress\",  \"0.0.0.0\")\n",
    "    .config(\"spark.driver.port\",         \"45555\")\n",
    "    .config(\"spark.blockManager.port\",   \"45556\")\n",
    "\n",
    "    # фиксированные порты, если нужны\n",
    "\n",
    "    # .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "    # .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/path/to/log4j.properties\")\n",
    "    .getOrCreate())\n",
    "\n",
    "\n",
    "# spark = (SparkSession.builder\n",
    "#              .appName(\"k8s-test-app\")\n",
    "#     .master(\"k8s://https://kubernetes.docker.internal:6443\")\n",
    "#\n",
    "#     # образ и namespace\n",
    "#     .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "#     .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "#     .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# # .config(\"spark.kubernetes.client.watch.allowWatchBookmarks\", \"false\")\n",
    "#     .config(\"spark.submit.deployMode\", \"client\")\n",
    "#     # .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "#\n",
    "#\n",
    "#     # ресурсы\n",
    "#     .config(\"spark.executor.instances\", \"2\")\n",
    "#\n",
    "#     # драйвер: рекламируемый адрес (для исполнителей) и bind address (куда слушать)\n",
    "#     # .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "#     # .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "#     #\n",
    "#     # # фиксированные порты, если нужны\n",
    "#     # .config(\"spark.driver.port\", \"45555\")\n",
    "#     # .config(\"spark.blockManager.port\", \"45556\")\n",
    "#\n",
    "#     .getOrCreate())\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "6ae5a99810e89538"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bad:",
   "id": "ee5bcae39cab8f21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "driver_host = \"localhost\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"KUBECONFIG\"] = os.path.join(ROOT_DIR, \"k8s-creds\", 'config')\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "             .appName(\"k8s-test-app\")\n",
    "    .master(\"k8s://https://kubernetes.docker.internal:6443\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "    .config(\"spark.submit.deployMode\", \"cluster\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "48a69e28b838c86b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Внешний IP или DNS ноды/LoadBalancer (замени на свой)\n",
    "spark_master_ip = 'host.docker.internal'\n",
    "spark_master_port = \"7077\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test-spark-k8s\") \\\n",
    "    .master(\"k8s://https://kubernetes.docker.internal:6443\")  \\\n",
    "    .config(\"spark.driver.host\", \"host.docker.internal\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", \"spark\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "acfbdea7f8082eaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!curl http://host.docker.internal:10005/",
   "id": "e7525cb8651819cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
