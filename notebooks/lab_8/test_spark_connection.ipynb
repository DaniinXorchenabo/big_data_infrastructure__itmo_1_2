{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Spark deploying",
   "id": "930bcbfb61c45a28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Without helm",
   "id": "7a699774d8167751"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!docker-compose -f .\\docker\\entrypoint.docker-compose.yml  -f .\\docker\\spark\\spark.docker-compose.yml --env-file=./env/.env  up producer -d --force-recreate\n",
    "\n",
    "!wsl -d <Distributive-Name>\n",
    "!kubectl create namespace spark\n",
    "!kubectl apply -f ./docker/kubernetes/deprecated/no-auth-spark-master.yaml -n spark\n",
    "!kubectl apply -f ./docker/kubernetes/deprecated/no-auth-spark-master.yaml -n spark"
   ],
   "id": "ac738fb865754e09",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With helm",
   "id": "7b9094065038f4be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```bash\n",
    "docker-compose -f .\\docker\\entrypoint.docker-compose.yml  -f .\\docker\\spark\\spark.docker-compose.yml --env-file=./env/.env  up producer -d --force-recreate\n",
    "\n",
    "wsl -d <Distributive-Name>\n",
    "```\n",
    "\n",
    "If it's helm creating:\n",
    "```bash\n",
    "helm repo add bitnami https://charts.bitnami.com/bitnami\n",
    "helm search repo bitnami\n",
    "helm install kayvan-release oci://registry-1.docker.io/bitnamicharts/spark\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "helm upgrade kayvan-release bitnami/spark \\\n",
    "  -f ./docker/kubernetes/helm/spark-config.yaml \\\n",
    "  --set image.tag=3.5.6 \\\n",
    "  --force\n",
    "kubectl apply -f ./docker/kubernetis/helm/spark-master-service.yaml\n",
    "```\n",
    "\n",
    "Some diagnostic commands:\n",
    "```bash\n",
    "kubectl get svc -n default\n",
    "kubectl get pod -n default\n",
    "kubectl get nodes -o wide\n",
    "kubectl get pods --show-labels\n",
    "curl host.docker.internal:30080\n",
    "```"
   ],
   "id": "578e68bdc9a5c1e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connection tests into spark pods",
   "id": "c64e534dc91146e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Correct:",
   "id": "7b07f03f74ce5551"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HOME\"] = \"/tmp\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/bitnami/python/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/opt/bitnami/python/bin/python3\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1) Создаём SparkSession, указываем master как spark://<имя-сервиса>:7077\n",
    "#    При работе внутри Pod’а Kubernetes автоматически резолвит \"spark-master\" в его ClusterIP.\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "        .appName(\"standalone-simple\")\n",
    "        # .master(\"spark://kayvan-release-spark-master-0:7077\")\n",
    "    .master(\"spark://host.docker.internal:30077\")\n",
    "        .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "        .config(\"spark.executor.instances\", \"2\")      # сколько Executors запустить\n",
    "        .config(\"spark.executor.cores\", \"1\")          # по одному CPU‐ядру\n",
    "        .config(\"spark.executor.memory\", \"1g\")      # по 512 МБ памяти\n",
    "        .config(\"spark.driver.memory\", \"1g\")        # драйверу тоже ограничим RAM, если нужно\n",
    "\n",
    "        .config(\"spark.submit.deployMode\", \"client\")\n",
    "        # .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "        # .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "        # .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "\n",
    "        # .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "\n",
    "    # .config(\"spark.driver.host\",         \"spark-master\")\n",
    "        .config(\"spark.driver.bindAddress\",  \"0.0.0.0\")\n",
    "        .config(\"spark.driver.port\",         \"45555\")\n",
    "        .config(\"spark.blockManager.port\",   \"45556\")\n",
    "\n",
    "\n",
    "    .config(\"spark.pyspark.python\", \"/opt/bitnami/python/bin/python3\")              # путь к python3 внутри контейнера\n",
    ".config(\"spark.executorEnv.PYSPARK_PYTHON\", \"/opt/bitnami/python/bin/python3\")  # тоже самое для среды executor’а\n",
    "\n",
    "        .getOrCreate()\n",
    ")\n",
    "\n",
    "# 2) Делаем простую проверку\n",
    "df = spark.range(1_000)                 # создаём DataFrame с числами от 0 до 999 999\n",
    "result = df.selectExpr(\"sum(id) as total\")  # суммируем колонку \"id\"\n",
    "result.show()                               # показываем в консоли: ожидаем 499999500000\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bad:",
   "id": "360367565100aa31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "# driver_host = \"localhost\"\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join(ROOT_DIR, \"k8s-creds\", 'config')\n",
    "# os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "# --conf spark.executor.memory=1G\n",
    "# --conf spark.executor.cores=1\n",
    "# pyspark-shell\n",
    "# \"\"\"\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"k8s-test-app\")\n",
    "     # .master(\"spark://localhost:7077\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    # .config(\"spark.driver.host\", \"localhost\")\n",
    "    .getOrCreate())\n",
    "\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "4383a4ad544c6ada"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "os.environ[\"HOME\"] = \"/tmp\"\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join('root', 'kube','config', 'config')\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join('k8s-creds', 'config')\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    SparkSession.builder.getOrCreate().stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"jupyter-on-spark\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    # .master(\"spark://https://kubernetes.default.svc:443\")\n",
    "    .config(\"spark.submit.deployMode\", \"client\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "    .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "    .getOrCreate()\n",
    ")\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()"
   ],
   "id": "f17fc4d9034b3b64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# import sys\n",
    "# from operator import add\n",
    "#\n",
    "# from pyspark.sql import SparkSession\n",
    "#\n",
    "#\n",
    "# spark = SparkSession\\\n",
    "#         .builder\\\n",
    "#         .appName(\"PythonWordCount\")\\\n",
    "#      .master(\"spark://spark-master:7077\")\\\n",
    "#         .getOrCreate()\n",
    "#\n",
    "# lines = spark.read.text(\"Привет Привет привет\").rdd.map(lambda r: r[0])\n",
    "# counts = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "#                   .map(lambda x: (x, 1)) \\\n",
    "#                   .reduceByKey(add)\n",
    "# output = counts.collect()\n",
    "# for (word, count) in output:\n",
    "#     print(\"%s: %i\" % (word, count))\n",
    "#\n",
    "# spark.stop()"
   ],
   "id": "a5fb4a391a3eaa9a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Connection test into jupyter docker image",
   "id": "b9fb77dd26578dfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Correct:",
   "id": "3cf683df298c2b69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "ROOT_DIR = '/workspace/NN'\n",
    "os.chdir(ROOT_DIR)\n",
    "from pyspark.sql import SparkSession\n",
    "# driver_host = \"localhost\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join(ROOT_DIR, \"k8s-creds\", 'config')\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"k8s-test-app\")\n",
    "    # .master(\"k8s://https://kubernetes.docker.internal:6443\")\n",
    ".master(\"spark://host.docker.internal:30077\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"default\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "\n",
    "        .config(\"spark.executor.instances\", \"2\")      # сколько Executors запустить\n",
    "        .config(\"spark.executor.cores\", \"1\")          # по одному CPU‐ядру\n",
    "        .config(\"spark.executor.memory\", \"1g\")      # по 512 МБ памяти\n",
    "        .config(\"spark.driver.memory\", \"1g\")        # драйверу тоже ограничим RAM, если нужно\n",
    "\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.caCertFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"ca.crt\"))\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.clientKeyFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"client.key\"))\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.clientCertFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"client.crt\"))\n",
    "    # .config(\"spark.kubernetes.executor.podNamePrefix\", \"spark-exec\")\n",
    "\n",
    "    .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "    .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "\n",
    "    # .config(\"spark.submit.deployMode\", \"cluster\")\n",
    "\n",
    "    .config(\"spark.driver.bindAddress\",  \"0.0.0.0\")\n",
    "    .config(\"spark.driver.port\",         \"45555\")\n",
    "    .config(\"spark.blockManager.port\",   \"45556\")\n",
    "\n",
    "    # фиксированные порты, если нужны\n",
    "\n",
    "    # .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "    # .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/path/to/log4j.properties\")\n",
    "    .getOrCreate())\n",
    "\n",
    "\n",
    "# spark = (SparkSession.builder\n",
    "#              .appName(\"k8s-test-app\")\n",
    "#     .master(\"k8s://https://kubernetes.docker.internal:6443\")\n",
    "#\n",
    "#     # образ и namespace\n",
    "#     .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "#     .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "#     .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# # .config(\"spark.kubernetes.client.watch.allowWatchBookmarks\", \"false\")\n",
    "#     .config(\"spark.submit.deployMode\", \"client\")\n",
    "#     # .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "#\n",
    "#\n",
    "#     # ресурсы\n",
    "#     .config(\"spark.executor.instances\", \"2\")\n",
    "#\n",
    "#     # драйвер: рекламируемый адрес (для исполнителей) и bind address (куда слушать)\n",
    "#     # .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "#     # .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "#     #\n",
    "#     # # фиксированные порты, если нужны\n",
    "#     # .config(\"spark.driver.port\", \"45555\")\n",
    "#     # .config(\"spark.blockManager.port\", \"45556\")\n",
    "#\n",
    "#     .getOrCreate())\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "6ae5a99810e89538"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bad:",
   "id": "ee5bcae39cab8f21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "driver_host = \"localhost\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"KUBECONFIG\"] = os.path.join(ROOT_DIR, \"k8s-creds\", 'config')\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "             .appName(\"k8s-test-app\")\n",
    "    .master(\"k8s://https://kubernetes.docker.internal:6443\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "    .config(\"spark.submit.deployMode\", \"cluster\")\n",
    "    .config(\"spark.executor.instances\", \"2\")\n",
    "    .getOrCreate())\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "48a69e28b838c86b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Внешний IP или DNS ноды/LoadBalancer (замени на свой)\n",
    "spark_master_ip = 'host.docker.internal'\n",
    "spark_master_port = \"7077\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"test-spark-k8s\") \\\n",
    "    .master(\"k8s://https://kubernetes.docker.internal:6443\")  \\\n",
    "    .config(\"spark.driver.host\", \"host.docker.internal\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", \"spark\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "acfbdea7f8082eaa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!curl http://host.docker.internal:10005/",
   "id": "e7525cb8651819cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
