{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T17:33:31.729090Z",
     "start_time": "2025-06-07T17:32:46.169537Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install pyspark==3.5.6 --upgrade",
   "id": "7892cacbbbae22e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.6\r\n",
      "  Downloading pyspark-3.5.6.tar.gz (317.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.4/317.4 MB\u001B[0m \u001B[31m33.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting py4j==0.10.9.7 (from pyspark==3.5.6)\r\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pyspark: filename=pyspark-3.5.6-py2.py3-none-any.whl size=317895856 sha256=86c472be7bbdd71c72337c420b1483e6fd57adf713b0405331664fbdd2c9e003\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/62/f3/ec15656ea4ada0523cae62a1827fe7beb55d3c8c87174aad4a\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: py4j, pyspark\r\n",
      "  Attempting uninstall: py4j\r\n",
      "    Found existing installation: py4j 0.10.9.9\r\n",
      "    Uninstalling py4j-0.10.9.9:\r\n",
      "      Successfully uninstalled py4j-0.10.9.9\r\n",
      "  Attempting uninstall: pyspark\r\n",
      "    Found existing installation: pyspark 4.0.0\r\n",
      "    Uninstalling pyspark-4.0.0:\r\n",
      "      Successfully uninstalled pyspark-4.0.0\r\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.6\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T20:01:32.058373Z",
     "start_time": "2025-06-09T20:01:31.602064Z"
    }
   },
   "cell_type": "code",
   "source": "!java -version",
   "id": "aad58b062108f9e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.27\" 2025-04-15\r\n",
      "OpenJDK Runtime Environment (build 11.0.27+6-post-Debian-1deb11u1)\r\n",
      "OpenJDK 64-Bit Server VM (build 11.0.27+6-post-Debian-1deb11u1, mixed mode, sharing)\r\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-09T20:24:51.741860Z",
     "start_time": "2025-06-09T20:24:50.417666Z"
    }
   },
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %reload_ext autoreload\n",
    "\n",
    "import os\n",
    "ROOT_DIR = '/workspace/NN'\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join(ROOT_DIR, \"k8s-creds\", 'config')\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "MONGO_USER = os.environ[\"MONGO_USER\"]\n",
    "MONGO_PASS = os.environ[\"MONGO_PASSWORD\"]\n",
    "MONGO_HOST = \"host.docker.internal\"\n",
    "# \"mongodb-svc.default.svc.cluster.local\"\n",
    "# \"mongodb-svc\"\n",
    "#  \"mongodb-svc1.default.svc.cluster.local\"\n",
    "MONGO_PORT = \"30717\"\n",
    "MONGO_ADDR = f\"{MONGO_USER}:{MONGO_PASS}@{MONGO_HOST}:{MONGO_PORT}\"\n",
    "MONGO_ADDR"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'admin:password@host.docker.internal:30717'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T20:24:53.655527Z",
     "start_time": "2025-06-09T20:24:53.612313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "def spark_app_generator(name):\n",
    "    spark = (SparkSession.builder\n",
    "        .appName(name)\n",
    "        .master(\"spark://host.docker.internal:30077\")\n",
    "        .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "        .config(\"spark.kubernetes.namespace\", \"default\")\n",
    "        .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "        .config(\"spark.executor.instances\", \"2\")      # сколько Executors запустить\n",
    "        .config(\"spark.executor.cores\", \"1\")          # по одному CPU‐ядру\n",
    "        .config(\"spark.executor.memory\", \"1g\")      # по 512 МБ памяти\n",
    "        .config(\"spark.driver.memory\", \"1g\")        # драйверу тоже ограничим RAM, если нужно\n",
    "        .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "        .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "        .config(\"spark.driver.bindAddress\",  \"0.0.0.0\")\n",
    "        .config(\"spark.driver.port\",         \"45555\")\n",
    "        .config(\"spark.blockManager.port\",   \"45556\")\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0,com.redislabs:spark-redis_2.12:3.1.0\")\n",
    "        .config(\"spark.mongodb.read.connection.uri\", f\"mongodb://{MONGO_ADDR}\")\n",
    "        .config(\"spark.mongodb.write.connection.uri\", f\"mongodb://{MONGO_ADDR}\")\n",
    "        .getOrCreate())\n",
    "    return spark\n",
    "\n",
    "# spark = spark_app_generator(\"k8s_test_from_external_docker\")\n",
    "# df = spark.range(1000)\n",
    "# df.selectExpr(\"sum(id)\").show()\n",
    "#\n",
    "# spark.stop()\n"
   ],
   "id": "a44cb19ce201662",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T20:25:26.912120Z",
     "start_time": "2025-06-09T20:24:56.511840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import (StructType, StructField,\n",
    "                               StringType, IntegerType, MapType,\n",
    "                               DoubleType\n",
    "                               )\n",
    "\n",
    "# Пример определения схемы. Настройте схему под структуру ваших данных.\n",
    "custom_schema = StructType([\n",
    "    StructField(\"_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "\n",
    "])\n",
    "\n",
    "spark = spark_app_generator('k8s_mongo_test')\n",
    "df = spark.read.schema(custom_schema).format(\"mongodb\") \\\n",
    "  .options(host=f\"{MONGO_HOST}:{MONGO_PORT}\", database=\"off\", collection='products').load()  # , database=\"off\", collection='products'\n",
    "\n",
    "'''\n",
    "mongosh \\\n",
    "  --username admin \\\n",
    "  --password password \\\n",
    "  --authenticationDatabase \"admin\"\n",
    "'''\n",
    "\n",
    "# Просмотр схемы и первых строк\n",
    "# print(df.count())\n",
    "df.printSchema()\n",
    "df.show(20)"
   ],
   "id": "d2a1348c502428bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /tmp/.ivy2/cache\n",
      "The jars for the packages stored in: /tmp/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      "com.redislabs#spark-redis_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9e83ac17-03cb-4e46-8a4e-2c9de93f55cc;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.3.0 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.8.2 in central\n",
      "\t[4.8.2] org.mongodb#mongodb-driver-sync;[4.8.1,4.8.99)\n",
      "\tfound org.mongodb#bson;4.8.2 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.8.2 in central\n",
      "\tfound org.mongodb#bson-record-codec;4.8.2 in central\n",
      "\tfound com.redislabs#spark-redis_2.12;3.1.0 in central\n",
      "\tfound redis.clients#jedis;3.9.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 11098ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.redislabs#spark-redis_2.12;3.1.0 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.mongodb#bson;4.8.2 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.8.2 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.8.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.3.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\tredis.clients#jedis;3.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   1   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tServer access error at url https://repos.spark-packages.org/org/mongodb/mongodb-driver-sync/maven-metadata.xml (javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake)\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9e83ac17-03cb-4e46-8a4e-2c9de93f55cc\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/8ms)\n",
      "25/06/09 20:25:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|         _id|        product_name|\n",
      "+------------+--------------------+\n",
      "|            |                NULL|\n",
      "|    00000000|           erytritol|\n",
      "|000000000054|Limonade artisana...|\n",
      "|000000000063|Mozzarella Schnit...|\n",
      "|000000000114|       Chocolate n 3|\n",
      "|    00000001|Wild Norwegian El...|\n",
      "|  0000000105|Paleta gran reser...|\n",
      "|    00000002|Filets de poulet ...|\n",
      "|    00000003|    Creatine Gummies|\n",
      "|    00000004|               Oreos|\n",
      "|  0000000475|Confiture extra c...|\n",
      "|    00000005|6K Protein White ...|\n",
      "|    00000006|                NULL|\n",
      "| 00000006666|      Gemüse - Gurke|\n",
      "|    00000007|  Ah Mini-maiswafels|\n",
      "|    00000008|                    |\n",
      "|    00000009|Mission Carb Bala...|\n",
      "|    00000010| Natual Energy Boost|\n",
      "|    00000011|                    |\n",
      "|    00000012|HIGH PROTEIN ICED...|\n",
      "+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:46:23.881695Z",
     "start_time": "2025-06-09T19:45:54.155970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "\n",
    "# spark = spark_app_generator('test_mongo_data_analysis')\n",
    "df = spark.read.schema(custom_schema).format(\"mongodb\") \\\n",
    "  .options(host=f\"{MONGO_HOST}:27017\", database=\"off\", collection='products').load()\n",
    "print(df.count())\n",
    "# Выбор числовых признаков\n",
    "# numeric_features = [ '_id', 'product_name',\n",
    "#                       'ingredients_n', 'ingredients_sweeteners_n', 'scans_n', 'additives_n',\n",
    "# 'energy_100g',\n",
    "# 'proteins_100g',\n",
    "# 'carbohydrates_100g',\n",
    "# 'fat_100g',\n",
    "\n",
    "# 'sugars_100g',\n",
    "# 'saturated-fat_100g',\n",
    "# 'salt_100g',\n",
    "# 'sodium_100g',\n",
    "# ]\n",
    "# Удаление строк с пропущенными значениями\n",
    "# df_clean = df.select(numeric_features).dropna()\n",
    "print(df.count())"
   ],
   "id": "8d22ab0128d89bf1",
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o131.count.\n: com.mongodb.spark.sql.connector.exceptions.MongoSparkException: Partitioning failed. Partitioner calling collStats command failed\n\tat com.mongodb.spark.sql.connector.read.MongoInputPartitionHelper.generateMongoBatchPartitions(MongoInputPartitionHelper.java:79)\n\tat com.mongodb.spark.sql.connector.read.MongoBatch.planInputPartitions(MongoBatch.java:53)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputPartitions$lzycompute(BatchScanExec.scala:59)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputPartitions(BatchScanExec.scala:59)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:179)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:175)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.supportsColumnar(BatchScanExec.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:162)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:171)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:186)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:186)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3615)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.mongodb.spark.sql.connector.exceptions.MongoSparkException: Partitioner calling collStats command failed\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.storageStats(PartitionerHelper.java:119)\n\tat com.mongodb.spark.sql.connector.read.partitioner.SamplePartitioner.generatePartitions(SamplePartitioner.java:94)\n\tat com.mongodb.spark.sql.connector.read.MongoInputPartitionHelper.generateMongoBatchPartitions(MongoInputPartitionHelper.java:52)\n\t... 82 more\nCaused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=mongodb-svc.default.svc.cluster.local:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: mongodb-svc.default.svc.cluster.local}, caused by {java.net.UnknownHostException: mongodb-svc.default.svc.cluster.local}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:184)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:46)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:133)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:90)\n\tat com.mongodb.client.internal.MongoIterableImpl.first(MongoIterableImpl.java:101)\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.lambda$storageStats$0(PartitionerHelper.java:109)\n\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.withCollection(AbstractMongoConfig.java:210)\n\tat com.mongodb.spark.sql.connector.config.ReadConfig.withCollection(ReadConfig.java:45)\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.storageStats(PartitionerHelper.java:105)\n\t... 84 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mPy4JJavaError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 22\u001B[39m\n\u001B[32m      4\u001B[39m df = spark.read.schema(custom_schema).format(\u001B[33m\"\u001B[39m\u001B[33mmongodb\u001B[39m\u001B[33m\"\u001B[39m) \\\n\u001B[32m      5\u001B[39m   .options(host=\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mMONGO_HOST\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m:27017\u001B[39m\u001B[33m\"\u001B[39m, database=\u001B[33m\"\u001B[39m\u001B[33moff\u001B[39m\u001B[33m\"\u001B[39m, collection=\u001B[33m'\u001B[39m\u001B[33mproducts\u001B[39m\u001B[33m'\u001B[39m).load()\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Выбор числовых признаков\u001B[39;00m\n\u001B[32m      8\u001B[39m \u001B[38;5;66;03m# numeric_features = [ '_id', 'product_name',\u001B[39;00m\n\u001B[32m      9\u001B[39m \u001B[38;5;66;03m#                       'ingredients_n', 'ingredients_sweeteners_n', 'scans_n', 'additives_n',\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Удаление строк с пропущенными значениями\u001B[39;00m\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m# df_clean = df.select(numeric_features).dropna()\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m22\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[43mdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/dataframe.py:1240\u001B[39m, in \u001B[36mDataFrame.count\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1217\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcount\u001B[39m(\u001B[38;5;28mself\u001B[39m) -> \u001B[38;5;28mint\u001B[39m:\n\u001B[32m   1218\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001B[39;00m\n\u001B[32m   1219\u001B[39m \n\u001B[32m   1220\u001B[39m \u001B[33;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1238\u001B[39m \u001B[33;03m    3\u001B[39;00m\n\u001B[32m   1239\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1240\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_jdf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcount\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001B[39m, in \u001B[36mJavaMember.__call__\u001B[39m\u001B[34m(self, *args)\u001B[39m\n\u001B[32m   1316\u001B[39m command = proto.CALL_COMMAND_NAME +\\\n\u001B[32m   1317\u001B[39m     \u001B[38;5;28mself\u001B[39m.command_header +\\\n\u001B[32m   1318\u001B[39m     args_command +\\\n\u001B[32m   1319\u001B[39m     proto.END_COMMAND_PART\n\u001B[32m   1321\u001B[39m answer = \u001B[38;5;28mself\u001B[39m.gateway_client.send_command(command)\n\u001B[32m-> \u001B[39m\u001B[32m1322\u001B[39m return_value = \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1323\u001B[39m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1325\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[32m   1326\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[33m\"\u001B[39m\u001B[33m_detach\u001B[39m\u001B[33m\"\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001B[39m, in \u001B[36mcapture_sql_exception.<locals>.deco\u001B[39m\u001B[34m(*a, **kw)\u001B[39m\n\u001B[32m    177\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdeco\u001B[39m(*a: Any, **kw: Any) -> Any:\n\u001B[32m    178\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m179\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    180\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    181\u001B[39m         converted = convert_exception(e.java_exception)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/local/lib/python3.12/site-packages/py4j/protocol.py:326\u001B[39m, in \u001B[36mget_return_value\u001B[39m\u001B[34m(answer, gateway_client, target_id, name)\u001B[39m\n\u001B[32m    324\u001B[39m value = OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[32m2\u001B[39m:], gateway_client)\n\u001B[32m    325\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[32m1\u001B[39m] == REFERENCE_TYPE:\n\u001B[32m--> \u001B[39m\u001B[32m326\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[32m    327\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    328\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name), value)\n\u001B[32m    329\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    330\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[32m    331\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[33m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.\n\u001B[32m    332\u001B[39m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[33m\"\u001B[39m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m, name, value))\n",
      "\u001B[31mPy4JJavaError\u001B[39m: An error occurred while calling o131.count.\n: com.mongodb.spark.sql.connector.exceptions.MongoSparkException: Partitioning failed. Partitioner calling collStats command failed\n\tat com.mongodb.spark.sql.connector.read.MongoInputPartitionHelper.generateMongoBatchPartitions(MongoInputPartitionHelper.java:79)\n\tat com.mongodb.spark.sql.connector.read.MongoBatch.planInputPartitions(MongoBatch.java:53)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputPartitions$lzycompute(BatchScanExec.scala:59)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.inputPartitions(BatchScanExec.scala:59)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar(DataSourceV2ScanExecBase.scala:179)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2ScanExecBase.supportsColumnar$(DataSourceV2ScanExecBase.scala:175)\n\tat org.apache.spark.sql.execution.datasources.v2.BatchScanExec.supportsColumnar(BatchScanExec.scala:36)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Strategy.apply(DataSourceV2Strategy.scala:162)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)\n\tat scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1431)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:70)\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:171)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:164)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:186)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:186)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3615)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: com.mongodb.spark.sql.connector.exceptions.MongoSparkException: Partitioner calling collStats command failed\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.storageStats(PartitionerHelper.java:119)\n\tat com.mongodb.spark.sql.connector.read.partitioner.SamplePartitioner.generatePartitions(SamplePartitioner.java:94)\n\tat com.mongodb.spark.sql.connector.read.MongoInputPartitionHelper.generateMongoBatchPartitions(MongoInputPartitionHelper.java:52)\n\t... 82 more\nCaused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=mongodb-svc.default.svc.cluster.local:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: mongodb-svc.default.svc.cluster.local}, caused by {java.net.UnknownHostException: mongodb-svc.default.svc.cluster.local}}]\n\tat com.mongodb.internal.connection.BaseCluster.getDescription(BaseCluster.java:184)\n\tat com.mongodb.internal.connection.SingleServerCluster.getDescription(SingleServerCluster.java:46)\n\tat com.mongodb.client.internal.MongoClientDelegate.getConnectedClusterDescription(MongoClientDelegate.java:144)\n\tat com.mongodb.client.internal.MongoClientDelegate.createClientSession(MongoClientDelegate.java:101)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.getClientSession(MongoClientDelegate.java:291)\n\tat com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:183)\n\tat com.mongodb.client.internal.MongoIterableImpl.execute(MongoIterableImpl.java:133)\n\tat com.mongodb.client.internal.MongoIterableImpl.iterator(MongoIterableImpl.java:90)\n\tat com.mongodb.client.internal.MongoIterableImpl.first(MongoIterableImpl.java:101)\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.lambda$storageStats$0(PartitionerHelper.java:109)\n\tat com.mongodb.spark.sql.connector.config.AbstractMongoConfig.withCollection(AbstractMongoConfig.java:210)\n\tat com.mongodb.spark.sql.connector.config.ReadConfig.withCollection(ReadConfig.java:45)\n\tat com.mongodb.spark.sql.connector.read.partitioner.PartitionerHelper.storageStats(PartitionerHelper.java:105)\n\t... 84 more\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:29:23.108036Z",
     "start_time": "2025-06-09T19:29:23.101225Z"
    }
   },
   "cell_type": "code",
   "source": "f\"mongodb://{MONGO_ADDR}\"",
   "id": "84e2f4c2800fdc1c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mongodb://mongodb-svc:27017'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!mongosh  \"mongodb://admin:password@mongodb-svc:27017\"\n",
    "!mongosh --username \"admin\"  --password \"password\""
   ],
   "id": "2471471fc9403c82"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
