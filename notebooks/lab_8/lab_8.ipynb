{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T17:33:31.729090Z",
     "start_time": "2025-06-07T17:32:46.169537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import time\n",
    "!pip install pyspark==3.5.6 --upgrade"
   ],
   "id": "7892cacbbbae22e0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark==3.5.6\r\n",
      "  Downloading pyspark-3.5.6.tar.gz (317.4 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m317.4/317.4 MB\u001B[0m \u001B[31m33.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hCollecting py4j==0.10.9.7 (from pyspark==3.5.6)\r\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\r\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\r\n",
      "Building wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for pyspark: filename=pyspark-3.5.6-py2.py3-none-any.whl size=317895856 sha256=86c472be7bbdd71c72337c420b1483e6fd57adf713b0405331664fbdd2c9e003\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/64/62/f3/ec15656ea4ada0523cae62a1827fe7beb55d3c8c87174aad4a\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: py4j, pyspark\r\n",
      "  Attempting uninstall: py4j\r\n",
      "    Found existing installation: py4j 0.10.9.9\r\n",
      "    Uninstalling py4j-0.10.9.9:\r\n",
      "      Successfully uninstalled py4j-0.10.9.9\r\n",
      "  Attempting uninstall: pyspark\r\n",
      "    Found existing installation: pyspark 4.0.0\r\n",
      "    Uninstalling pyspark-4.0.0:\r\n",
      "      Successfully uninstalled pyspark-4.0.0\r\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.6\r\n",
      "\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-07T20:44:15.597499Z",
     "start_time": "2025-06-07T20:44:14.207814Z"
    }
   },
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# %reload_ext autoreload\n",
    "\n",
    "import os\n",
    "ROOT_DIR = '/workspace/NN'\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---------------",
   "id": "cb13637f8d1cf667"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T20:44:32.944307Z",
     "start_time": "2025-06-07T20:44:22.458930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import os\n",
    "ROOT_DIR = '/workspace/NN'\n",
    "os.chdir(ROOT_DIR)\n",
    "from pyspark.sql import SparkSession\n",
    "# driver_host = \"localhost\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "# os.environ[\"KUBECONFIG\"] = os.path.join(ROOT_DIR, \"k8s-creds\", 'config')\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = f\"\"\"\n",
    "--conf spark.executor.memory=1G\n",
    "--conf spark.executor.cores=1\n",
    "pyspark-shell\n",
    "\"\"\"\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"k8s-test-app\")\n",
    "    # .master(\"k8s://https://kubernetes.docker.internal:6443\")\n",
    ".master(\"spark://host.docker.internal:30077\")\n",
    "    .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "    .config(\"spark.kubernetes.namespace\", \"default\")\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "\n",
    "        .config(\"spark.executor.instances\", \"2\")      # сколько Executors запустить\n",
    "        .config(\"spark.executor.cores\", \"1\")          # по одному CPU‐ядру\n",
    "        .config(\"spark.executor.memory\", \"1g\")      # по 512 МБ памяти\n",
    "        .config(\"spark.driver.memory\", \"1g\")        # драйверу тоже ограничим RAM, если нужно\n",
    "\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.caCertFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"ca.crt\"))\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.clientKeyFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"client.key\"))\n",
    "    # .config(\"spark.kubernetes.authenticate.submission.clientCertFile\",  os.path.join(ROOT_DIR, 'k8s-creds', \"client.crt\"))\n",
    "    # .config(\"spark.kubernetes.executor.podNamePrefix\", \"spark-exec\")\n",
    "\n",
    "    .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "    .config(\"spark.jars.ivy\", \"/tmp/.ivy2\")  # укажи директорию вручную\n",
    "\n",
    "    # .config(\"spark.submit.deployMode\", \"cluster\")\n",
    "\n",
    "    .config(\"spark.driver.bindAddress\",  \"0.0.0.0\")\n",
    "    .config(\"spark.driver.port\",         \"45555\")\n",
    "    .config(\"spark.blockManager.port\",   \"45556\")\n",
    "\n",
    "    # фиксированные порты, если нужны\n",
    "\n",
    "    # .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.ui.showConsoleProgress\", \"true\")\n",
    "    # .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:/path/to/log4j.properties\")\n",
    "    .getOrCreate())\n",
    "\n",
    "\n",
    "# spark = (SparkSession.builder\n",
    "#              .appName(\"k8s-test-app\")\n",
    "#     .master(\"k8s://https://kubernetes.docker.internal:6443\")\n",
    "#\n",
    "#     # образ и namespace\n",
    "#     .config(\"spark.kubernetes.container.image\", \"bitnami/spark:latest\")\n",
    "#     .config(\"spark.kubernetes.namespace\", \"spark\")\n",
    "#     .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "# # .config(\"spark.kubernetes.client.watch.allowWatchBookmarks\", \"false\")\n",
    "#     .config(\"spark.submit.deployMode\", \"client\")\n",
    "#     # .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "#\n",
    "#\n",
    "#     # ресурсы\n",
    "#     .config(\"spark.executor.instances\", \"2\")\n",
    "#\n",
    "#     # драйвер: рекламируемый адрес (для исполнителей) и bind address (куда слушать)\n",
    "#     # .config(\"spark.driver.host\", \"host.docker.internal\")\n",
    "#     # .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "#     #\n",
    "#     # # фиксированные порты, если нужны\n",
    "#     # .config(\"spark.driver.port\", \"45555\")\n",
    "#     # .config(\"spark.blockManager.port\", \"45556\")\n",
    "#\n",
    "#     .getOrCreate())\n",
    "\n",
    "df = spark.range(1000)\n",
    "df.selectExpr(\"sum(id)\").show()\n",
    "\n",
    "spark.stop()\n"
   ],
   "id": "a44cb19ce201662",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/07 20:44:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[Stage 0:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|sum(id)|\n",
      "+-------+\n",
      "| 499500|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
