version: '3.9'

services:
  spark-master:
    image: bitnami/spark:latest
    container_name: spark-master
    user: root
    volumes:
      - type: bind
        source: ../neural/datasets
        target: /workspace/NN/neural/datasets
      - type: bind
        source: ../neural/weights
        target: /opt/bitnami/spark/work/labs
#      - hadoop_conf:/opt/bitnami/spark/conf/hadoop-conf:ro   # Монтируем HDFS‑конфиги в Spark :contentReference[oaicite:5]{index=5}
      - type: bind
        source: ../docker/spark/hadoop
        target: /opt/hadoop/etc/hadoop
      - type: bind
        source: ../docker/spark/run.sh
        target: /opt/bitnami/scripts/spark/run.sh
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_USER=root
      - PYSPARK_PYTHON=/opt/bitnami/python/bin/python3
    ports:
      - "0.0.0.0:8080:8080"   # веб-интерфейс Spark Master
      - "0.0.0.0:7077:7077"   # порт Spark Master
      - "0.0.0.0:4040:4040"
    networks:
      spark-network:
        aliases:
          - spark-master

  spark-worker-1:
    image: bitnami/spark:latest
    container_name: spark-worker-1
    user: root
    volumes:
      - type: bind
        source: ../neural/datasets
        target: /workspace/NN/neural/datasets
      - type: bind
        source: ../neural/weights
        target: /opt/bitnami/spark/work/labs
#      - hadoop_conf:/opt/bitnami/spark/conf/hadoop-conf:ro   # Монтируем HDFS‑конфиги в Spark :contentReference[oaicite:5]{index=5}
      - type: bind
        source: ../docker/spark/hadoop
        target: /opt/hadoop/etc/hadoop
      - type: bind
        source: ../docker/spark/run.sh
        target: /opt/bitnami/scripts/spark/run.sh
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_USER=root
      - PYSPARK_PYTHON=/opt/bitnami/python/bin/python3
    depends_on:
      - spark-master
    networks:
      spark-network:
        aliases:
          - spark-worker-1

  spark-worker-2:
    image: bitnami/spark:latest
    container_name: spark-worker-2
    user: root
    volumes:
      - type: bind
        source: ../neural/datasets
        target: /workspace/NN/neural/datasets
      - type: bind
        source: ../neural/weights
        target: /opt/bitnami/spark/work/labs
#      - hadoop_conf:/opt/bitnami/spark/conf/hadoop-conf:ro   # Монтируем HDFS‑конфиги в Spark :contentReference[oaicite:5]{index=5}
      - type: bind
        source: ../docker/spark/hadoop
        target: /opt/hadoop/etc/hadoop
      - type: bind
        source: ../docker/spark/run.sh
        target: /opt/bitnami/scripts/spark/run.sh
    environment:
      - HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_USER=root
      - PYSPARK_PYTHON=/opt/bitnami/python/bin/python3
    depends_on:
      - spark-master
    networks:
      spark-network:
        aliases:
          - spark-worker-2

  producer:
    image: daniinxorchenabo/itmo-prod-dl-labs-env:lighting-cpu-dev-latest
    build:
#      context: ...
#      dockerfile: ../docker/lighting.Dockerfile
      target: dev_build

    container_name: producer-8
    command: ./docker/before_learn.sh
    env_file:
      - ../env/.env
    volumes:
      - type: bind
        source: ../docker/jupyter_config
        target: /root/.jupyter/
      - type: bind
        source: ..
        target: /workspace/NN

    ports:
      - "0.0.0.0:8889:8888"
      - "0.0.0.0:45556:45556"
      - "0.0.0.0:45555:45555"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
#    depends_on:
#      - spark-master
    networks:
      spark-network:
        aliases:
          - producer

  mongodb:
    image: mongo:6-jammy
    container_name: mongodb
    restart: always
    ports:
      - '0.0.0.0:27017:27017'
      - '0.0.0.0:28017:28017'
    volumes:
      - type: bind
        source: ../neural/datasets/lab_5/mongo
        target: /data/db
    networks:
      - spark-network
    environment:
      MONGO_INITDB_ROOT_USERNAME: ${MONGO_USER}
      MONGO_INITDB_ROOT_PASSWORD: ${MONGO_PASSWORD}

  redis:
    image: redis:6.2-alpine
    ports:
      - "0.0.0.0:6379:6379"
    volumes:
      - redis_data:/data
    restart: always
    networks:
      - spark-network

  # Neo4j — промежуточное хранилище очищенных узлов
  neo4j:
    image: neo4j:5-enterprise
    container_name: neo4j
    networks:
      - spark-network
    environment:
      NEO4J_AUTH: "${NEO4J_USER}/${NEO4J_PASSWORD}"
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
    ports:
      - "0.0.0.0:7474:7474"    # HTTP Web UI
      - "0.0.0.0:7687:7687"    # Bolt
    volumes:
      - neo4j_data:/data

# 6. Data Mart (витрина данных) на Scala/Spark
  data-mart:
    build:
      context: ../docker/spark/data-mart
      dockerfile: Dockerfile
    container_name: data-mart

    env_file:
      - ../env/.env
    networks:
      - spark-network
    depends_on:
      - spark-master
      - mongodb
      - neo4j
    environment:
#      - MONGO_URI=mongodb://admin:password@mongo:27017/off?authSource=admin
      - NEO4J_URL=bolt://neo4j:7687
      - NEO4J_USER=${NEO4J_USER}
      - NEO4J_PASS=${NEO4J_PASSWORD}
      - MONGO_URI=mongodb://${MONGO_USER}:${MONGO_PASSWORD}@mongodb:27017  # /off?authSource=admin
      - TARGET_COLLECTION=cleaned_products

#      - TARGET_PATH=hdfs://datanode:9000/data-mart
#    volumes:
#      - hadoop_conf:/opt/hadoop/etc/hadoop:ro

volumes:
#  namenode_data:
#  datanode1_data:
#  datanode2_data:
#  hadoop_conf:
  redis_data:
  neo4j_data:

networks:
  spark-network:
    driver: bridge
    internal: false
    name: "${CONTAINER_PREFIX:-unsetted_prefix}-spark-network"
